{"pages":[{"title":"Categories","date":"2019-02-23T13:37:58.003Z","path":"categories/index.html","text":""},{"title":"About","date":"2019-02-23T13:37:58.002Z","path":"about/index.html","text":""},{"title":"Tags","date":"2019-02-23T13:37:58.004Z","path":"tags/index.html","text":""}],"posts":[{"title":"函数式接口-FunctionalInterface","date":"2019-03-09T03:55:57.000Z","path":"wiki/程序语言/Java/函数式接口-FunctionalInterface/","text":"什么是函数式接口其实之前在讲Lambda表达式的时候提到过，所谓的函数式接口，当然首先是一个接口，然后就是在这个接口里面只能有一个抽象方法。 这种类型的接口也称为SAM接口，即Single Abstract Method interfaces。 ### 用途它们主要用在Lambda表达式和方法引用（实际上也可认为是Lambda表达式）上 12345@FunctionalInterfaceinterface GreetingService &#123; void sayMessage(String message);&#125; 那么就可以使用Lambda表达式来表示该接口的一个实现(注：JAVA 8 之前一般是用匿名类实现的)： 1GreetingService greetService1 = message -&gt; System.out.println(\"Hello \" + message); 关于@FunctionalInterface注解Java 8为函数式接口引入了一个新注解@FunctionalInterface，主要用于编译级错误检查，加上该注解，当你写的接口不符合函数式接口定义的时候，编译器会报错。 提醒：加不加@FunctionalInterface对于接口是不是函数式接口没有影响，该注解知识提醒编译器去检查该接口是否仅包含一个抽象方法 函数式接口里允许定义默认方法函数式接口里是可以包含默认方法，因为默认方法不是抽象方法，其有一个默认实现，所以是符合函数式接口的定义的； 123456789101112131415@FunctionalInterfaceinterface GreetingService&#123; void sayMessage(String message); default void doSomeMoreWork1() &#123; // Method body &#125; default void doSomeMoreWork2() &#123; // Method body &#125;&#125; JDK中的函数式接口举例java.lang.Runnable,java.awt.event.ActionListener,java.util.Comparator,java.util.concurrent.Callablejava.util.function包下的接口，如Consumer、Predicate、Supplier等 Java 8 函数式接口 - Functional Interface","tags":[{"name":"Java","slug":"Java","permalink":"http://wiki.quartz.ren/tags/Java/"}],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Java","slug":"程序语言/Java","permalink":"http://wiki.quartz.ren/categories/程序语言/Java/"}]},{"title":"Netty的NioEventLoop","date":"2019-03-08T17:13:34.000Z","path":"wiki/MicroService/技术框架/Netty/Netty的NioEventLoop/","text":"NioEventLoop1234567891011121314151617181920212223public final class NioEventLoop extends SingleThreadEventLoop &#123; private Selector selector; private Selector unwrappedSelector; private SelectedSelectionKeySet selectedKeys; private final SelectorProvider provider; /** * Boolean that controls determines if a blocked Selector.select should * break out of its selection process. In our case we use a timeout for * the select method and the select method will block for that time unless * waken up. */ private final AtomicBoolean wakenUp = new AtomicBoolean(); private final SelectStrategy selectStrategy; private volatile int ioRatio = 50; private int cancelledKeys; private boolean needsToSelectAgain; ...&#125; NioEventLoop 是Netty的核心类。 NioEventLoop 的首要职责就是为注册在它上的 channels 服务，发现这些 channels 上发生的新连接事件、读写等 I/O 事件，然后将事件转交 channel 流水线处理。 NioEventLoop 的职责 发现策略SelectStrategy selectStrategy12345678final class DefaultSelectStrategy implements SelectStrategy &#123; static final SelectStrategy INSTANCE = new DefaultSelectStrategy(); private DefaultSelectStrategy() &#123; &#125; @Override public int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception &#123; return hasTasks ? selectSupplier.get() : SelectStrategy.SELECT; &#125;&#125;","tags":[{"name":"Netty","slug":"Netty","permalink":"http://wiki.quartz.ren/tags/Netty/"}],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"},{"name":"技术框架","slug":"MicroService/技术框架","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/"},{"name":"Netty","slug":"MicroService/技术框架/Netty","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/Netty/"}]},{"title":"FastText","date":"2019-03-06T14:55:57.000Z","path":"wiki/机器学习/NLP/FastText/","text":"ngramfastText可以在词向量的训练和句子分类上取得非常好的表现，尤其表现在对罕见词进行字符粒度上的处理。 每个单词除了单词本身外还被表示为多个字符级别的n-grams(有时候也称为N元模子) 例如对于单词 matter. 当n=3时， fasttext对该词对字符ngram就表示为 1&lt;ma, mat, att, tte, ter, er&gt; 其中&lt; 和 &gt; 是作为边界符号被添加，来将一个单词的ngrams与单词本身区分开来。 再举一个例子，如果单词mat属于我们的词汇表，则会被表示为 . 这么做刚好让一些短词以其他词的ngram出现，有助于更好的学习到这些短词的含义。 从本质上讲，这可以帮助你捕捉后缀/前缀的含义。 minn,maxn可以通过-minn和-maxn这两个参数来控制ngrams的长度，这两个标志分别决定了ngrams的最小和最大字符数，也即控制了ngrams的范围。 这个模型被认为是一个词袋模型，因为除了用于选择n-gram的滑动窗口外,它并没有考虑对单词的内部结构进行特征选择。 它只要求字符落在窗口以内，但并不关心ngrams的顺序。 你可以将这两个值都设为0来完全关闭n-gram. 也就是不产生n-gram符号，单纯用单词作为输入. 当您的模型中的“单词”不是特定语音的单词时或者说字符级别的n-gram没有意义的时候，这会变得很有用。最常见的例子是当您将id作为您的单词输入。 在模型更新期间，fastText会学习到每个ngram以及整个单词符号的权重。 读取数据虽然fastText的训练是多线程的，但是读取数据却是通过单线程来完成。而文本解析和分词则在读取输入数据时就被完成了。让我们来看看具体是怎么做到的: FastText通过-input参数获取一个文件句柄用于输入数据。FastText不支持从stdin读取数据，它初始化两个向量word2int_和words_来跟踪输入信息。 word2int_是一个字符串到数值的映射集，索引键是单词字符串，根据字符串哈希值可以得到一个数值作为它的值，同时这个数值恰好就对应到了words_数组(std:::vector)的索引。 words_数组在读取输入时根据单词出现的顺序递增创建索引，每个索引对应的值是一个结构体entry，这个entry封装了单词的所有信息。条目包含以下信息: 123456struct entry &#123; std::string word; int64_t count; entry_type type; std::vector&lt;int32_t&gt; subwords;&#125;; https://baijiahao.baidu.com/s?id=1606667200878009384&amp;wfr=spider&amp;for=pc","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"},{"name":"NLP","slug":"机器学习/NLP","permalink":"http://wiki.quartz.ren/categories/机器学习/NLP/"}]},{"title":"简单线性回归实现","date":"2019-03-03T15:55:57.000Z","path":"wiki/机器学习/机器学习100天/002.简单线性回归实现/","text":"第一步：数据预处理12345678910import pandas as pdimport numpy as npimport matplotlib.pyplot as pltdataset = pd.read_csv('studentscores.csv')X = dataset.iloc[ : , : 1 ].valuesY = dataset.iloc[ : , 1 ].valuesfrom sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 1/4, random_state = 0) 第二步：训练集使用简单线性回归模型来训练123from sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor = regressor.fit(X_train, Y_train) 第三步：预测结果1Y_pred = regressor.predict(X_test) 第四步：可视化123456789101112131415161718192021222324252627import pandas as pdimport numpy as npimport matplotlib.pyplot as pltfrom sklearn.model_selection import KFoldfrom sklearn.model_selection import train_test_splitdataset = pd.read_csv('~/Documents/100-Days-Of-ML-Code/datasets/studentscores.csv')X = dataset.iloc[ : , : 1 ].valuesY = dataset.iloc[ : , 1 ].valuesX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 1/4, random_state = 0) from sklearn.linear_model import LinearRegressionregressor = LinearRegression()regressor = regressor.fit(X_train, Y_train)Y_pred = regressor.predict(X_test)plt.scatter(X_train , Y_train, color = 'red')plt.plot(X_train , regressor.predict(X_train), color ='blue')plt.show()plt.scatter(X_test , Y_test, color = 'red')plt.plot(X_test , regressor.predict(X_test), color ='blue')plt.show()","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"},{"name":"机器学习100天","slug":"机器学习/机器学习100天","permalink":"http://wiki.quartz.ren/categories/机器学习/机器学习100天/"}]},{"title":"数据预处理","date":"2019-02-28T15:55:57.000Z","path":"wiki/机器学习/机器学习100天/001.数据预处理/","text":"导入需要的包numpy, pandas 导入数据集数据集通常为.csv格式. 使用Pandas的read_csv方法读取本地csv文件为一个数据帧。 然后，从数据帧中制作自变量和因变量的矩阵和向量。 处理丢失数据集大多数据集是不完整的，为了不降低机器学习模型的性能，需要处理数据。 可以使用整列的平均值或中间值替换丢失的数据。我们使用sklean.preprocssing库中的Imputer类完成这项任务。 解析分类数据分类数据指的是含有标签值而不是数字值的变量。取值范围通常是固定的。例如”Yes”和“No”.不能用于模型的数学计算，所以需要解析成数字。 为了实现这一功能，我们从sklearn.preprocesing库中导入LabelEncoder类。 拆分数据集为测试集合和训练集合一般比例为80:20 导入sklearn.crossvalidation库中的train_test_split()方法。 特征缩放大部分模型算法使用两点间的欧式距离表示，但此特征在幅度、单位和范围姿态问题上变化很大。 在距离计算中，高幅度的特征比低幅度特征权重更大。可用特征标准化或Z值归一化解决。 导入sklearn.preprocessing库中的StandardScalar类。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#Day 1: Data Prepocessing#Step 1: Importing the librariesimport numpy as npimport pandas as pd#Step 2: Importing datasetdataset = pd.read_csv('../datasets/Data.csv')X = dataset.iloc[ : , :-1].valuesY = dataset.iloc[ : , 3].valuesprint(\"Step 2: Importing dataset\")print(\"X\")print(X)print(\"Y\")print(Y)#Step 3: Handling the missing datafrom sklearn.preprocessing import Imputerimputer = Imputer(missing_values = \"NaN\", strategy = \"mean\", axis = 0)imputer = imputer.fit(X[ : , 1:3])X[ : , 1:3] = imputer.transform(X[ : , 1:3])print(\"---------------------\")print(\"Step 3: Handling the missing data\")print(\"step2\")print(\"X\")print(X)#Step 4: Encoding categorical datafrom sklearn.preprocessing import LabelEncoder, OneHotEncoderlabelencoder_X = LabelEncoder()X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0])#Creating a dummy variableonehotencoder = OneHotEncoder(categorical_features = [0])X = onehotencoder.fit_transform(X).toarray()labelencoder_Y = LabelEncoder()Y = labelencoder_Y.fit_transform(Y)print(\"---------------------\")print(\"Step 4: Encoding categorical data\")print(\"X\")print(X)print(\"Y\")print(Y)#Step 5: Splitting the datasets into training sets and Test setsfrom sklearn.model_selection import train_test_splitX_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size = 0.2, random_state = 0)print(\"---------------------\")print(\"Step 5: Splitting the datasets into training sets and Test sets\")print(\"X_train\")print(X_train)print(\"X_test\")print(X_test)print(\"Y_train\")print(Y_train)print(\"Y_test\")print(Y_test)#Step 6: Feature Scalingfrom sklearn.preprocessing import StandardScalersc_X = StandardScaler()X_train = sc_X.fit_transform(X_train)X_test = sc_X.transform(X_test)print(\"---------------------\")print(\"Step 6: Feature Scaling\")print(\"X_train\")print(X_train)print(\"X_test\")print(X_test)","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"},{"name":"机器学习100天","slug":"机器学习/机器学习100天","permalink":"http://wiki.quartz.ren/categories/机器学习/机器学习100天/"}]},{"title":"在看的书和文章","date":"2019-02-24T01:56:24.000Z","path":"wiki/生活学习/在看的书和文章/","text":"个人博客 Docker — 从入门到实践 学习java设计模式 设计模式之禅（第2版） hadoop-notebook gitee-pages 有趣的文章Web应用架构演进及系统性能、稳定性所需要解决的问题 dubbo学习过程、使用经验分享及实现原理简单介绍","tags":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/tags/生活学习/"}],"categories":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/categories/生活学习/"}]},{"title":"规则引擎入门","date":"2019-02-24T01:56:24.000Z","path":"wiki/工具组件/Drools/001.规则引擎入门/","text":"Drools Documentation Drools概述业务决策自动化 任何一家组织或企业都面临着作出决定或决策性的问题，这些问题通常比较复杂，一般都需要算法分析大量决策相关的数据，例如当前一些热门的话题，如物联网、人工智能、认知计算都基于算法分析海量数据，产生有用信息的过程。Drools 作为一个开源的推理引擎，有 20 年历史，基于规则匹配算法非常适用与与这一场景。 用于业务规则管理，业务资源优化和复杂事件处理（CEP）。 构建一个全面的业务自动化平台 语法见Documennt 复杂规则编写: 条件元素，比较运算符,两种dialect的比较 CEP复杂时间处理. 复杂时间是多事件的事件处理概念，目标是在事件集合（事件流，事件云）中识别用户定义的有意义事件，CEP采用诸如检测许多事件的复杂模式，事件关联和抽象以及事件层的流程。 丢失包裹检测 规则设计要求： 设计规则检测如果一个包裹在通过 CHECK_IN 位置 10 分钟以后是否通过 SORTING 位置，如果否则说明规则丢失。 包裹数统计 编写规则统计最近一小时内经过 SORTING 位置的包裹总数。 包裹重量统计 编写规则统计统计过去通过 CHECK_IN 的 5 个包裹的平均重量。 统计包裹从 CHECK_IN 到 SORTING 的平均时间 包裹扫描系统需要统计过去 5 个连续的包裹从 CHECK_IN 到 SORTING 的平均处理时间。 Entry Point 插入事件 编辑规则可以获取不同位置的包裹。 规则整合通过不同的方式来管理获取规则 优化问题LABS本部分通过实验来验证 Drools 相关的理论及概念。 drools-examples doc drools-examples github 业务规则的定义抽象.规则有哪些元素组成. 多个规则的智行顺序和规则管理","tags":[{"name":"规则引擎","slug":"规则引擎","permalink":"http://wiki.quartz.ren/tags/规则引擎/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Drools","slug":"工具组件/Drools","permalink":"http://wiki.quartz.ren/categories/工具组件/Drools/"}]},{"title":"201902","date":"2019-02-24T01:56:24.000Z","path":"wiki/生活学习/TODOLIST/201902/","text":"20190224","tags":[{"name":"todo","slug":"todo","permalink":"http://wiki.quartz.ren/tags/todo/"}],"categories":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/categories/生活学习/"},{"name":"TODOLIST","slug":"生活学习/TODOLIST","permalink":"http://wiki.quartz.ren/categories/生活学习/TODOLIST/"}]},{"title":"TODOLIST","date":"2019-02-24T01:56:24.000Z","path":"wiki/生活学习/TODOLIST/README/","text":"一周计划 一道算法题 一篇英语文章 一个技术小技巧 一个观点分享 把握好机会，别人给你的机会。要看懂一些事情。不要装作什么都不知道。 不要说一些没有用的话，还不如不说","tags":[{"name":"todo","slug":"todo","permalink":"http://wiki.quartz.ren/tags/todo/"}],"categories":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/categories/生活学习/"},{"name":"TODOLIST","slug":"生活学习/TODOLIST","permalink":"http://wiki.quartz.ren/categories/生活学习/TODOLIST/"}]},{"title":"ubunut 挂载samba","date":"2019-02-24T01:56:24.000Z","path":"wiki/软件工程/Linux/ubuntu挂载samba/","text":"ubunut 挂载smbubuntu18.04挂载smblinux挂载samba文件系统的方法12sudo apt install cifs-utilssudo mount -t cifs -o username=xxx,password=xxx //$&#123;ip&#125;/deploy/download /mnt/download sslocal -s 108.160.129.222 -p 25002 -k ‘2018vultr@lgqsb’ -l 1080 -t 600 -m aes-256-cfb -d $1 –pid-file /tmp/vpn-agent.pid –log-file /tmp/vpn-agent.log export GIO_EXTRA_MODULES=/usr/lib/x86_64-linux-gnu/gio/modules/ if [ $1 == ‘stop’ ];then gsettings set org.gnome.system.proxy mode ‘none’ echo ‘stop 1232.’else gsettings set org.gnome.system.proxy.http host ‘127.0.0.1’ gsettings set org.gnome.system.proxy.http port 1080 gsettings set org.gnome.system.proxy mode ‘manual’ echo ‘start 22222.’fi","tags":[{"name":"linux","slug":"linux","permalink":"http://wiki.quartz.ren/tags/linux/"}],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"Linux","slug":"软件工程/Linux","permalink":"http://wiki.quartz.ren/categories/软件工程/Linux/"}]},{"title":"","date":"2019-02-23T13:37:58.002Z","path":"wiki/软件工程/问题/关于非技术类的几个问题/","text":"创业的流程 别人的一些经历,经验,需要学习的地方,思想 有关管理,交流的问题,人的管理.","tags":[],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"问题","slug":"软件工程/问题","permalink":"http://wiki.quartz.ren/categories/软件工程/问题/"}]},{"title":"","date":"2019-02-23T13:37:57.987Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES权限设置/","text":"参考 开启权限 1curl -H &quot;Content-Type:application/json&quot; -XPOST http://yun.quartz.ren:9200/_xpack/license/start_trial?acknowledge=true 修改es的配置 12添加xpack.security.enabled: true 设置用户名和密码详见 1bin/elasticsearch-setup-passwords interactive 重要: 在Kibana中配置Security 添加用户,角色等操作. 6.3版本x-pack破解","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"HBase伪集群模式安装","date":"2019-02-16T04:56:24.000Z","path":"wiki/数据处理/组件安装配置/Hadoop组件安装/3.HBase伪集群模式安装/","text":"同Hadoop安装，到CDH版下载地址下载hbase。选择 hbase-1.0.0-cdh5.4.0.tar.gz hbase伪分布式模式是基于hdfs环境的 因此，在安装hadoop的前提下，我们配置Hbase的伪分布式模式如下： 1.下载并解压 2.配置环境变量（可选） 123456vi ~/.bashrc#添加export PATH=$PATH:/opt/app/skyeye/hbase-1.0.0-cdh5.4.0/bin. ~/.bashrc#验证hbase version 伪集群模式配置 1.配置 conf/hbase-env.sh将JAVA_HOME变量设置为本机jdk路径。如下： 12export JAVA_HOME=/opt/tools/jdk1.8.0_131 #配置本机的java安装根目录export HBASE_MANAGES_ZK=true #配置使用hbase自带的zookeeper，不使用自己搭建的zookeeper 如果使用 export HBASE_MANAGES_ZK=true， 即配置不使用hbase自带的zookeeper，使用自己搭建的zookeeper hbase可以使用自定义zookeeper管理，也可以使用自带的zookeeper。 2.配置conf/hbase-site.xml修改hbase.rootdir,将其指向hdfs，并指定Hbase在HDFS上的存储路径。将hbase.cluster.distributed设置为true。添加zk的节点地址。如下： 1234567891011121314151617181920&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!--以下信息只有在使用自己搭建的zk时添加--&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;cdhnode1,cdhnode2,cdhnode3&lt;/value&gt; &lt;/property&gt; &lt;!--默认为/tmp/目录下--&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/app/skyeye/data/hbasetmp/&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.启动HBase 完成上述操作之后，启动HBase，需要先启动Hadoop。 启动及检查： 123456789zbm@node3:~$ start-all.sh zbm@node3:~$ jps9250 ResourceManager9683 Jps9365 NodeManager9110 SecondaryNameNode8935 DataNode8795 NameNode 上述则Hadoop启动成功。 当前版本。使用start-all.sh启动hadoop时会提示以下信息，所以建议分别启动hdfs和yarn。 1This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh 之后启动HBase，启动及检查 123456789101112start-hbase.sh # 启动Hbasezbm@node3:~$ jps # 检查3728 Jps3123 HRegionServer2407 NodeManager1961 DataNode2298 ResourceManager3004 HMaster1852 NameNode2158 SecondaryNameNode2943 HQuorumPeer 以上几个进程说明Hbase启动成功。 4.Hbase操作 可以进入shell模式，通过命令行操作查看hbase数据库信息。 12345hbase shell #进入shell模式list # 查看当前数据库所有表信息describe &apos;member&apos; # 查看表结构# 创建一个member表，其拥有member_id,address,info三个列族create &apos;member&apos;,&apos;member_id&apos;,&apos;address&apos;,&apos;info&apos; 5.查看HDFS的HBase数据库文件 123456789zbm@node3:～$ hadoop fs -ls /hbase17/10/28 20:45:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableFound 6 itemsdrwxr-xr-x - nova supergroup 0 2017-10-28 00:03 /hbase/.tmpdrwxr-xr-x - nova supergroup 0 2017-10-28 00:03 /hbase/WALsdrwxr-xr-x - nova supergroup 0 2017-10-28 00:03 /hbase/data-rw-r--r-- 3 nova supergroup 42 2017-10-28 00:03 /hbase/hbase.id-rw-r--r-- 3 nova supergroup 7 2017-10-28 00:03 /hbase/hbase.versiondrwxr-xr-x - nova supergroup 0 2017-10-28 00:14 /hbase/oldWALs 6.停止HBase 1stop-hbase.sh #停止Hbase 问题：停止hbase时，HRegionServer进程不能停止。会导致下次启动hbase时出错。 原因: HBase的用户界面 yarn: http://192.168.1.20:8088/cluster hdfs状态: http://192.168.1.20:50070/dfshealth.html#tab-overview Master: http://192.168.1.20:60010/master.jsp","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://wiki.quartz.ren/tags/hadoop/"},{"name":"hbase","slug":"hbase","permalink":"http://wiki.quartz.ren/tags/hbase/"}],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"组件安装配置","slug":"数据处理/组件安装配置","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/"},{"name":"Hadoop组件安装","slug":"数据处理/组件安装配置/Hadoop组件安装","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/Hadoop组件安装/"}]},{"title":"常用操作命令与问题记录","date":"2019-02-16T04:56:24.000Z","path":"wiki/数据处理/组件安装配置/Hadoop组件安装/4.常用操作与问题记录/","text":"启动hadoop 12start-dfs.shstart-yarn.sh 停止hadoop 12stop-dfs.shstop-yarn.sh 启动或停止所有，即：HDFS和Yarn 问题记录一、dfs的端口9000不通原因：忘记格式化hdfs。 启动顺序 1.启动hdfs 2.启动yarn 3.启动hbase 二、查看hbase在hdfs中目录时使用hadoop fs -ls /hbase查看hbase目录，出现以下信息： 117/10/28 22:25:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable hbase 可以配置使用本地文件系统（一般单击模式），也可以使用hdfs（伪集群或集群模式） hbase 可以配置使用自己安装的zk，也可以使用自带的zk。 hadoop的name、data文件夹下存放什么？","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://wiki.quartz.ren/tags/hadoop/"},{"name":"hbase","slug":"hbase","permalink":"http://wiki.quartz.ren/tags/hbase/"}],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"组件安装配置","slug":"数据处理/组件安装配置","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/"},{"name":"Hadoop组件安装","slug":"数据处理/组件安装配置/Hadoop组件安装","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/Hadoop组件安装/"}]},{"title":"伪集群模式安装","date":"2019-02-16T04:56:24.000Z","path":"wiki/数据处理/组件安装配置/Hadoop组件安装/2.伪集群模式安装/","text":"搭建hadoop-2.6.0-cdh5.4.7伪分布式 CDH版下载地址 先来看看什么是CDH，为什么选择CHD版的Hadoop。 CDH属于Hadoop的一个发行版。 Hadoop有以下发行版： Apache Hadoop Cloudera’s Distribution Including Apache Hadoop（CDH） Hortonworks Data Platform (HDP) MapR EMR CDH版有以下优点： 版本划分清晰 版本更新速度快 支持Kerberos安全认证 文档清晰 支持多种安装方式（Cloudera Manager方式） 安装hadoop-2.6.0-cdh5.4.0首先到指定网站下载安装包CDH版下载地址 解压下载的安装包 配置伪集群 1、进入 hadoop-2.6.0-cdh5.4.0/etc/hadoop 2、编辑 hadoop-env.sh 1vi hadoop-env.sh 3、修改JAVA_HOME的配置为 1export JAVA_HOME=/opt/tools/jdk1.8.0_131 4、编辑core-site.xml,添加如下配置： 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node2:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; node2说明，如果没有配置hosts，请将node2换成IP地址:wp保存并退出。 5、编辑hdfs-site.xml,添加如下配置1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;!--开启web hdfs--&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/opt/cdh/hadoop/name&lt;/value&gt; &lt;description&gt; namenode 存放name table(fsimage)本地目录（需要修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.edits.dir&lt;/name&gt; &lt;value&gt;$&#123;dfs.namenode.name.dir&#125;&lt;/value&gt; &lt;description&gt;namenode存放 transactionfile(edits)本地目录（请自行修改）&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/opt/cdh/hadoop/data&lt;/value&gt; &lt;description&gt;datanode存放block本地目录（请自行修改）&lt;/description&gt; &lt;/property&gt;&lt;/configuration&gt; 以上配置完成，还需要创建文件夹 12mkdir -p cdh/hadoop/namemkdir cdh/hadoop/data 6、配置mapred-site.xml 12345678cp mapred-site.xml.template mapred-site.xml之后加入以下配置&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 7、编辑yarn-site.xml 1234567&lt;configuration&gt; &lt;!-- Site specific YARN configuration properties --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 到此，所有配置都已完成。 格式化HDFS1bin/hdfs namenode -format 看到如下信息： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263************************************************************/15/09/22 14:59:46 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]15/09/22 14:59:46 INFO namenode.NameNode: createNameNode [-format]15/09/22 14:59:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable15/09/22 14:59:49 WARN common.Util: Path /opt/cdh/hadoop/name should be specified as a URI in configuration files. Please update hdfs configuration.15/09/22 14:59:49 WARN common.Util: Path /opt/cdh/hadoop/name should be specified as a URI in configuration files. Please update hdfs configuration.Formatting using clusterid: CID-41ea6672-a32e-4b16-b704-962381ed409a15/09/22 14:59:49 INFO namenode.FSNamesystem: No KeyProvider found.15/09/22 14:59:49 INFO namenode.FSNamesystem: fsLock is fair:true15/09/22 14:59:49 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit=100015/09/22 14:59:49 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true15/09/22 14:59:49 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.00015/09/22 14:59:49 INFO blockmanagement.BlockManager: The block deletion will start around 2015 九月 22 14:59:4915/09/22 14:59:49 INFO util.GSet: Computing capacity for map BlocksMap15/09/22 14:59:49 INFO util.GSet: VM type = 64-bit15/09/22 14:59:49 INFO util.GSet: 2.0% max memory 966.7 MB = 19.3 MB15/09/22 14:59:49 INFO util.GSet: capacity = 2^21 = 2097152 entries15/09/22 14:59:50 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false15/09/22 14:59:50 INFO blockmanagement.BlockManager: defaultReplication = 115/09/22 14:59:50 INFO blockmanagement.BlockManager: maxReplication = 51215/09/22 14:59:50 INFO blockmanagement.BlockManager: minReplication = 115/09/22 14:59:50 INFO blockmanagement.BlockManager: maxReplicationStreams = 215/09/22 14:59:50 INFO blockmanagement.BlockManager: shouldCheckForEnoughRacks = false15/09/22 14:59:50 INFO blockmanagement.BlockManager: replicationRecheckInterval = 300015/09/22 14:59:50 INFO blockmanagement.BlockManager: encryptDataTransfer = false15/09/22 14:59:50 INFO blockmanagement.BlockManager: maxNumBlocksToLog = 100015/09/22 14:59:50 INFO namenode.FSNamesystem: fsOwner = root (auth:SIMPLE)15/09/22 14:59:50 INFO namenode.FSNamesystem: supergroup = supergroup15/09/22 14:59:50 INFO namenode.FSNamesystem: isPermissionEnabled = true15/09/22 14:59:50 INFO namenode.FSNamesystem: HA Enabled: false15/09/22 14:59:50 INFO namenode.FSNamesystem: Append Enabled: true15/09/22 14:59:50 INFO util.GSet: Computing capacity for map INodeMap15/09/22 14:59:50 INFO util.GSet: VM type = 64-bit15/09/22 14:59:50 INFO util.GSet: 1.0% max memory 966.7 MB = 9.7 MB15/09/22 14:59:50 INFO util.GSet: capacity = 2^20 = 1048576 entries15/09/22 14:59:50 INFO namenode.NameNode: Caching file names occuring more than 10 times15/09/22 14:59:50 INFO util.GSet: Computing capacity for map cachedBlocks15/09/22 14:59:50 INFO util.GSet: VM type = 64-bit15/09/22 14:59:50 INFO util.GSet: 0.25% max memory 966.7 MB = 2.4 MB15/09/22 14:59:50 INFO util.GSet: capacity = 2^18 = 262144 entries15/09/22 14:59:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.999000012874603315/09/22 14:59:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 015/09/22 14:59:50 INFO namenode.FSNamesystem: dfs.namenode.safemode.extension = 3000015/09/22 14:59:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 1015/09/22 14:59:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 1015/09/22 14:59:50 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,2515/09/22 14:59:50 INFO namenode.FSNamesystem: Retry cache on namenode is enabled15/09/22 14:59:50 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis15/09/22 14:59:50 INFO util.GSet: Computing capacity for map NameNodeRetryCache15/09/22 14:59:50 INFO util.GSet: VM type = 64-bit15/09/22 14:59:50 INFO util.GSet: 0.029999999329447746% max memory 966.7 MB = 297.0 KB15/09/22 14:59:50 INFO util.GSet: capacity = 2^15 = 32768 entries15/09/22 14:59:50 INFO namenode.NNConf: ACLs enabled? false15/09/22 14:59:50 INFO namenode.NNConf: XAttrs enabled? true15/09/22 14:59:50 INFO namenode.NNConf: Maximum size of an xattr: 1638415/09/22 14:59:51 INFO namenode.FSImage: Allocated new BlockPoolId: BP-314159059-192.168.1.3-144290519105615/09/22 14:59:51 INFO common.Storage: Storage directory /opt/cdh/hadoop/name has been successfully formatted.15/09/22 14:59:51 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid &gt;= 015/09/22 14:59:51 INFO util.ExitUtil: Exiting with status 015/09/22 14:59:51 INFO namenode.NameNode: SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at node2/192.168.1.3************************************************************/ 如果不报错，则格式化成功。 然后分别启动HDFS和Yarn： 12sbin/start-dfs.shsbin/start-yarn.sh 启动过程没有错误则启动成功。 验证 使用jps可以查看相关进程 显示如下： 1234567nova@ubuntu208:~$ jps7667 Jps28532 DataNode28742 SecondaryNameNode29319 NodeManager28376 NameNode29018 ResourceManager 管理地址 yarn: http://192.168.1.34:8088/cluster hdfs状态: http://192.168.1.34:50070/dfshealth.html#tab-overview","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://wiki.quartz.ren/tags/hadoop/"}],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"组件安装配置","slug":"数据处理/组件安装配置","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/"},{"name":"Hadoop组件安装","slug":"数据处理/组件安装配置/Hadoop组件安装","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/Hadoop组件安装/"}]},{"title":"Reactive","date":"2019-01-08T03:55:57.000Z","path":"wiki/编程相关/响应式编程/","text":"响应式编程基于事件驱动(事件模式，或者订阅者模式)，类似于Netty异步事件编程模型． 对不同的事件做不同的处理．所有信息都通过一个编程模型处理． 好处传统模型相比 JavaWeb开发，基于Servlet. Servlet3.0之前 线程阻塞模型，只有当业务处理完成并返回后时结束Servlet线程． 3.0规范新特性，支持异步处理－Servlet线程将耗时操作委派给另一个线程完成.在不生成响应的情况下返回至容器．（问题） Guava的EventBus实现订阅者模式，观察者模式 1234567891011121314151617public class EventBusDemo &#123; @Subscribe public void sendMessageByMail(String message) &#123; System.out.println(\"邮件发送一条信息：\" + message); &#125; @Subscribe public void sendMessageByPhone(String message) &#123; System.out.println(\"短信发送一条信息:\" + message); &#125; public static void main(String[] args) &#123; EventBus eventBus = new EventBus(); eventBus.register(new EventBusDemo()); eventBus.post(\"hi, boys\"); &#125;&#125; Mono和Flux常用API都是数据反应式编程的核心组件． Reactor是JVM的完全非阻塞响应式编程基础，具有高效的需求管理（以管理”背压”的形式）它直接与Java 8功能的API，特别是整合CompletableFuture，Stream和 Duration Flux 相当于一个 RxJava Observable 观察者 WebFluxSpring WebFlux是随Spring 5推出的响应式Web框架。 Spring WebFlux快速上手——响应式Spring的道法术器 微服务,部署包大小,应用占用内存大小. Rsocket用于响应式应用程序的新网络协议(应用层协议)． 提供Java，JavaScript，C ++和Kotlin等实现 它是一种基于Reactive Streams背压的双向，多路复用，基于消息的二进制协议 语言无关. 目的该协议专门设计用于与Reactive风格应用配合使用，这些应用程序基本上是非阻塞的，并且通常（但不总是）与异步行为配对 所谓Reactive背压: 即发布者无法向订户发送数据直到该订户已经准备就绪的想法，这是与“异步”的关键区别。（服务端主动） 问题: 很多个客户端对于同一个消息，准备好的时间层次不一，服务端怎么控制这个(这个消息需要一直保存着吗，什么时候清理)． 反应式编程(响应式reactive)是 Java 中高效应用的下一个前沿。但有两个主要障碍 -数据访问和网络。RSocket旨在解决后一个问题，而R2DBC旨在解决前者问题。 响应式应用新协议RSocket iPhone和Andriod手机，与后端服务交互，提供数据统计，所有这些互动模型，http并不是为此设计． http. 超文本传输协议,用于从WWW服务器传输超文本到本地浏览器的传输协议。它可以使浏览器更加高效，使网络传输减少. rest. 基于HTTP的REST服务. restTemplate. 获取http资源, 通过指定的一些格式(json等)． 数据统计收集 消息推送 异步响应 RSocket、. Envoy和. Istio从微服务治理的角度看RSocket、. Envoy和. Istio 重点是把反应流的实现，提升到应用层上来。其实在底层的协议中，就有反应流的实现，tcp的滑动窗口就是很好的例子。 很大一部分的线上故障是由于阻塞链接造成的. 简单的例子是如果所有的通讯都是反应式的，那就不用容断了. 与http不同的四种交互模式 （重点） Fire-and-Forget优化请求/响应，在不需要响应时非常有用，例如非关键事件日志记录。 请求/响应当您发送一个请求并收到一个响应时，就像HTTP一样。即使在这里，该协议也具有优于HTTP的优点，因为它是异步和多路复用的。 请求/流类似于返回集合的请求/响应，集合被回送而不是查询直到完成，因此例如发送银行帐号，用实时的帐户事务流进行响应。 频道允许任意交互模型的双向消息流。 Unix网络编程模型中，底层操作系统的通道都是全双工的，同时支持读写操作． 多路复用的Selector不短的轮询注册在其上的Channel.如果某个Channel上面发生读或者写事件，这个Channel就处于就绪状态，会被Selector轮询出来．然后通过SelectionKey可以获取就绪的Channel的集合，进行后续的I/O操作. TomcatNio 针对网络IO层面的异步－多路复用.(读写) Rsocket是交互模式的异步.(或者说Rsocket的请求和响应与Http的请求和响应有什么区别，优点在那里.) Http的异步通过callBack回调实现.(比如短信交互流程) 客户端使用http发送，短信平台和网关再到运营商都是长链接协议.短信平台受到同一链路上的送达之后．会callBack客户端.客户端收到后，处理回调．如果回调资源处理不当(处理不过来),会导致回调消息丢失． (Rsocket回压场景) 这种http的异步是通过应用程序多次请求实现.再就是客户端层面控制异步. 将请求后的等待丢进线程池或者队列来存储 AsyncCall，然后去做其他的事情． 将 AsycnCall 添加到队列中。将任务交给 Dispatcher 去执行。比如 OKHTTP实现的异步请求． 使用线程池处理异步任务（这种开销太大，很少做）.真正的异步执行者 AsyncCall 使用队列.将 AsycnCall 添加到队列中。将任务交给 Dispatcher 去执行 在使用 Dispatcher 会将 AsyncCall 交给指定的线程去执行，而 AsyncCall 是 NamedRunnable 的子类 OKHTTP异步和同步请求简单分析 Rsocket的异步，理解为没有收到响应，链接保持，但可以做其他事情，受到响应后再做处理.Rsocket天然支持? 深度解读Tomcat中的NIO模型 异步编程：协作性多任务处理","tags":[],"categories":[{"name":"编程相关","slug":"编程相关","permalink":"http://wiki.quartz.ren/categories/编程相关/"}]},{"title":"开发工具","date":"2019-01-08T03:55:57.000Z","path":"wiki/工具组件/IDE/开发工具/","text":"好用的开发工具dbeaver https://nosqlbooster.com/downloads","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"IDE","slug":"工具组件/IDE","permalink":"http://wiki.quartz.ren/categories/工具组件/IDE/"}]},{"title":"前端框架","date":"2019-01-08T03:55:57.000Z","path":"wiki/工具组件/前端方案/前端框架记录/","text":"https://antd-admin.zuiidea.com/zh/dashboard","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"前端方案","slug":"工具组件/前端方案","permalink":"http://wiki.quartz.ren/categories/工具组件/前端方案/"}]},{"title":"ssh免密登录","date":"2019-01-08T03:55:57.000Z","path":"wiki/数据处理/组件安装配置/Hadoop组件安装/1.配置ssh免密登录/","text":"1.首先查看电脑的SSH Keys是否存在 1ls -al ~/.ssh 2.存在以下文件则说明key已生成 12-rw------- 1 zbm zbm 1675 Oct 28 11:04 id_rsa-rw-r--r-- 1 zbm zbm 399 Oct 28 11:04 id_rsa.pub 3.否则生成key: 1ssh-keygen -t rsa -C &quot;your_email@example.com&quot; 4.之后配置免密登陆：即将pub key put到远程服务器。输入密码，之后每次就可免密登陆。 1ssh-copy-id -i ~/.ssh/id_rsa.pub &lt;romte_ip&gt;","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"组件安装配置","slug":"数据处理/组件安装配置","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/"},{"name":"Hadoop组件安装","slug":"数据处理/组件安装配置/Hadoop组件安装","permalink":"http://wiki.quartz.ren/categories/数据处理/组件安装配置/Hadoop组件安装/"}]},{"title":"ES介绍","date":"2018-12-15T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES介绍/","text":"Es介绍","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"Logstash安装","date":"2018-12-15T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/Logstash安装/","text":"logstash是做数据采集的，类似于flume。 官网logstash介绍 下载地址 解压后执行一下命令,查看效果: 1bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos; 安装logstash-input-jdbc参考 logstash-input-jdbc插件是logstash 的一个个插件。 使用ruby语言开发. 安装gem, 替换淘宝镜像 123456安装gemsudo apt install gemsudo apt install rubygem -vgem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/gem sources -l RubyGems镜像站 查看logstash可用插件 1bin/logstash-plugin list --verbose 以上找到对应的版本. 查看相应的文档https://www.elastic.co/guide/en/logstash-versioned-plugins/current/v4.3.9-plugins-inputs-jdbc.html 安装命令 1bin/logstash-plugin install logstash-input-jdbc 使用实现mysql数据同步到Elasticsearch 需要一个mysql驱动包，sql文件,以及conf配置文件 sql文件 bank_sync.sql 1234567SELECT t.id, t.`code`, t.`name`, t.per_day_limitFROM tb_bank_type t mysql.conf文件 12345678910111213141516171819202122232425262728293031323334353637input &#123; jdbc &#123; # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/carinsurance&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;carinsurance&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;/opt/elasticsearch/logstash-6.2.2/sql/mysql-connector-java-5.1.40.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; statement_filepath =&gt; &quot;/opt/elasticsearch/logstash-6.2.2/sql/bank_sync.sql&quot; schedule =&gt; &quot;*/1 * * * *&quot; type =&gt; &quot;jdbc&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;127.0.0.1:9200&quot; index =&gt; &quot;bank&quot; document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 上面配置文件中的sql文件和mysql驱动注意路径正确 启动logstash 123bin/logstash -f mysql.conf 后台启动: nohup ./logstash -f mysql.conf &gt; /dev/null 2&gt;&amp;1 &amp;nohup bin/logstash -f sync-data/mysql.conf &amp; 注意: es需要外网访问,同kibana,需要配置如下: 1network.host: 0.0.0.0 配置以上后出现问题: 系统最大文件描述符限制,最大虚拟内存限制 解决: 12345678vi /etc/security/limits.conf 将65535 改为65536root用户执行以下:vi /etc/sysctl.conf 添加一下配置vm.max_map_count=655360使其生效sysctl -p 应用【技术实验】mysql准实时同步数据到Elasticsearch 全文搜索引擎 Elasticsearch （三）logstash-input-jdbc同步数据 到elasticsearch","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"Redis常用操作","date":"2018-12-14T17:55:57.000Z","path":"wiki/数据处理/DataStore/Redis/Redis常用操作/","text":"Redis开发运维实践指南","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Redis","slug":"数据处理/DataStore/Redis","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Redis/"}]},{"title":"Docker 仓库搭建","date":"2018-12-14T13:55:57.000Z","path":"wiki/工具组件/Docker/5.Docker镜像仓库搭建/","text":"123456789101112mkdir -p /opt/data/registry //创建目录docker run -d \\ -p 5000:5000 \\ --restart=always \\ --name docker-registry \\ -v /data/docker-registry:/var/lib/registry \\ registry:2-d : 让容器可以后台运行-p ：指定映射端口（前者是宿主机的端口号，后者是容器的端口号）-v ：数据挂载（前者是宿主机的目录，后者是容器的目录）--name : 为运行的容器命名","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"系统镜像","date":"2018-12-14T13:55:57.000Z","path":"wiki/工具组件/Docker/7.系统镜像/","text":"Ubuntu镜像地址 拉取镜像 启动镜像 在镜像中操作1docker run -i -t ubuntu bash CentOS下载镜像1docker pull centos:6.8 启动容器1docker run -ti --name centos001 centos:6.8 /bin/bash 安装软件12345vi 、 sshdyum install viyum install openssh-server 查看是否启动 123/etc/init.d/sshd statusservice sshd start","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Ubuntu16安装Docker环境","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/1.Ubuntu16安装Docker环境/","text":"之前每次安装都是上网查查资料，每次待找半天。所以在这里总结下。首先Docker在Ubuntu下的安装分为以下几个步骤： 添加Docker源 安装aufs驱动linux-image-extra 安装Docker 安装后的设置 Docker更新 添加Docker源解释一下。使用Ubuntu在命令行可以直接安装Docker，但是一般都是老一些的版本，而且下载缓慢。所以换Docker源很有必要。 具体操作如下： 123456789101112131415sudo apt-get update# 增加CA证书sudo apt-get install apt-transport-https ca-certificates# 添加GPG Key(一种加密手段)sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D# 添加docker的源echo &quot;deb https://apt.dockerproject.org/repo ubuntu-xenial main&quot; &gt; /etc/apt/sources.list.d/docker.list或echo &quot;deb https://apt.dockerproject.org/repo ubuntu-xenial main&quot; | sudo tee /etc/apt/sources.list.d/docker.list# 再次更新源sudo apt-get update# 清除过时源（以防万一）sudo apt-get purge lxc-docker# 验证APT是从正确的库源下载应用apt-cache policy docker-engine 安装aufs驱动linux-image-extraFor Ubuntu Trusty, Wily, and Xenial, it’s recommended to install the linux-image-extra kernel package. The linux-image-extra package allows you use the aufs storage driver可以实现容器间可执行文件和运行库的共享。 1sudo apt-get install linux-image-extra-$(uname -r) 安装Docker12sudo apt-get updatesudo apt-get install docker-engine 安装后的设置将用户添加到docker组中，避免每次都是用sudo。 1sudo usermod -aG docker $&#123;user&#125; Docker更新1234# 更新Dockersudo apt-get upgrade docker-engine# 卸载Dockersudo apt-get purge docker-engine","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Docker环境Mysql使用","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/2.Docker环境使用MySQL/","text":"阿里云镜像仓库地址 配置docker使用阿里云镜像地址 获取镜像 1docker pull mysql 启动镜像 1docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=zhang -d mysql 参数指定root用户密码 验证 进入容器，登录mysql，可以看到123456789Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.7.20 MySQL Community Server (GPL)Copyright (c) 2000, 2017, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. mysql创建用户并授权1234create database ibase4j; create user ibase; grant all privileges on ibase4j.* to ibase@&apos;192.168.1.7&apos; identified by &apos;ibase&apos;; flush privileges;","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Dockerfile语法","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/4.Dockerfile语法/","text":"Dockerfile创建镜像 Dockerfile 12345678910111213141516171819202122232425# This dockerfile uses the ubuntu image# VERSION 2 - EDITION 1# Author: docker_user# Command format: Instruction [arguments / command] ..# Base image to use, this must be set as the first lineFROM ubuntu# Maintainer: docker_user &lt;docker_user at email.com&gt; (@docker_user)MAINTAINER docker_user docker_user@email.com# Commands to update the image#RUN echo &quot;deb http://archive.ubuntu.com/ubuntu/ raring main universe&quot; &gt;&gt; /etc/apt/sources.list#RUN apt-get update &amp;&amp; apt-get install -y nginx#RUN echo &quot;\\ndaemon off;&quot; &gt;&gt; /etc/nginx/nginx.conf# Commands when creating a new container#CMD /usr/sbin/nginxENV JAVA_HOME=/opt/tools/jdk1.8.0_131ENV PATH=$JAVA_HOME/bin:$PATHENV CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar# docker build -t nova/jdk .# docker run -it --name my-java -d java","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Docker环境其他中间件使用","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/3.Docker环境其他中间件使用/","text":"phpmyadmin 12docker run --name myadmin -d -e PMA_HOST=172.17.0.2 phpmyadmin/phpmyadmin docker run --name yst_myadmin -d -e PMA_HOST=10.10.73.109 phpmyadmin/phpmyadmin redis 1docker run --name some-redis -d redis elasticsearch 1docker run -d elasticsearch es的python 客户端 Elasticsearch Clients Elasticsearch集群和索引常用命令 rabbitmq 123docker run -d --hostname my-rabbit --name some-rabbit rabbitmqdocker run -d --hostname rabbit001 --name myrabbit -e RABBITMQ_DEFAULT_USER=admin \\-e RABBITMQ_DEFAULT_PASS=admin123 -p 5672:5672 rabbitmq zookeeper 1docker run --name some-zookeeper -p 2181:2181 --restart always -d zookeeper nexusnexus官方镜像 1docker pull registry.cn-hangzhou.aliyuncs.com/nichozuo/nexus 123456789docker run \\ --detach \\ --name nexus \\ --restart always \\ --publish 8081:8081 \\ --env CONTEXT_PATH=/nexus \\ --volume /opt/tools/nexus:/sonatype-work \\ --volume /etc/localtime:/etc/localtime:ro \\ registry.cn-hangzhou.aliyuncs.com/nichozuo/nexus 镜像启动见链接以上地址","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Docker NetWork","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/6.Docker网络/","text":"docker容器跨主机互联小实验 docker 多种跨主机访问选择哪一种 理解Docker跨多主机容器网络 Docker rabbit1docker run -d --hostname my-rabbit --name rabbit -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=password -p 15672:15672 -p 5672:5672 -p 25672:25672 -p 61613:61613 -p 1883:1883 rabbitmq:management Docker redis1docker run -d --name redis-server -p 6379:6379 redis --requirepass &quot;password&quot;","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"ELK","date":"2018-12-09T03:55:57.000Z","path":"wiki/数据处理/数据收集/ELK/","text":"Docker快速搭建elk服务镜像","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"数据收集","slug":"数据处理/数据收集","permalink":"http://wiki.quartz.ren/categories/数据处理/数据收集/"}]},{"title":"Python常用函数","date":"2018-12-09T03:55:57.000Z","path":"wiki/程序语言/Python/基础操作/","text":"shape读取矩阵的长度. shape函数是numpy.core.fromnumeric中的函数，它的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度。shape的输入参数可以是一个整数（表示维度），也可以是一个矩阵。以下例子可能会好理解一些： Python numpy函数：shape用法 mat创建矩阵 python中的mat的操作","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"其他算法","date":"2018-12-07T14:55:57.000Z","path":"wiki/机器学习/5.其他算法/","text":"Java中的字符串相似度","tags":[],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"Tcp,Ip及其应用","date":"2018-12-04T15:11:57.000Z","path":"wiki/软件工程/计算机网络/TCP-IP及应用问题/","text":"SocketRead问题针对线程处于 at java.net.SocketInputStream.socketRead0(Native Method) , java.lang.Thread.State: RUNNABLE. 123456789&quot;http-nio-10251-exec-1213&quot; #86023 daemon prio=5 os_prio=0 tid=0x00007f838c0f8000 nid=0x1f9 runnable [0x00007f83693bd000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)... 关于TCP交互流程与客户端服务端状态 TimeWait,CloseWait 有没有问题","tags":[],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"计算机网络","slug":"软件工程/计算机网络","permalink":"http://wiki.quartz.ren/categories/软件工程/计算机网络/"}]},{"title":"Kafka问题总结","date":"2018-12-04T15:11:57.000Z","path":"wiki/数据处理/DataStore/Kafka/关于最近kafka的几个问题/","text":"consumer提交offset失败使用spring-kafka, 配置auto.commit =true 会使用kafka.client的 自动提交机制(5秒钟提交一次-具体细节TODO). 然而一定时间取出的消息没有处理完,长时间没能提交成功??? 现象: offset没有提交成功, consumer的协调者处于dead状态, 恢复后但还能正常消费,但offset一直未能提交. Socket一个线程导致消费挂起现象: Socket一直处于read状态,导致往队列push消息失败,取出来的消息也没有处理, 没有提交offset. 可能1. socket read timeout 设置后, 其他线程就处于 Timewait 状态,但是 实际设置connect 超时时间在 socket 超时时间之后, 所以没有超时时间相当于.","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"机器学习认识","date":"2018-12-02T14:55:57.000Z","path":"wiki/机器学习/1.机器学习认识/","text":"传统上认为，让计算机完成某一件事情的唯一方法，就是详细的记录某个算法（就是一系列的指令，告诉计算机能做什么），并解释其如何运行。 但机器学习算法不一样：通过从数据中推断，计算机自己会弄明白该怎么做。掌握的数据越多，它们的工作就越顺利。现在我们不给计算机编程。它们自己给自己编程。 机器学习是以数据为依据，数据越多，能学的也越多、没有数据，那就什么也学不到。有了大数据？那就有太多的东西可以学习。 只要有足够的数据，一段只有几百行代码的程序可以轻易生成几百万代码的程序。而且可以不同问题持续的去编写不同的程序。 机器学习有时会和人工智能混淆。严格来说，机器学习是人工智能的子集，但机器学习发展如此壮大且成功，现已超越以前它引以为傲的母领域。人工智能的目标是教会计算机做现在人类能做的事，并且做的更好。而机器学习可以说就是其中最重要的事：不持续学习。计算机就永远无法跟上人类的步伐，有了学习，一切都与时俱进。 知乎-机器学习该怎么入门 机器学习入门资源不完全汇总","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"Logistic回归","date":"2018-12-02T14:55:57.000Z","path":"wiki/机器学习/3.Logistic回归/","text":"概述回归: 假设有一组数据点,用一条直线对这些点进行拟合,这个拟合过程就是回归.回归一词源于最佳拟合,表示要找到最佳拟合参数. 训练分类器的做法就是寻找最佳拟合参数. 使用的一些优化算法: 梯度上升法,最小二乘法. 最优化算法1. 最小二乘法向量运算进行参数求解过程 损失函数,给定数据X,Y, 根据aX 的出的Y1 与Y之间的偏差,称为损失.怎么将这个损失降到最低. 首先定义这损失,线性空间的距离,通过欧几里得距离定义这个损失. 损失最小化: 对损失函数求导,得到参数方程. 进行参数计算. 2. 梯度下降为什么梯度下降是必须的? 最小二乘法参数计算的问题: 矩阵是否满秩 运算性能 梯度下降不仅限于线性回归. 经过多次的重复, 比直接运算(参数计算)的优点.","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"Nlp认识","date":"2018-12-02T14:55:57.000Z","path":"wiki/机器学习/NLP/Nlp认识/","text":"七大NLP技术 技术1：文本嵌入（Text Embeddings）在传统的NLP中，我们将单词视为离散符号，然后可以用one-hot向量表示。向量的维度是整个词汇表中单词的数量。单词作为离散符号的问题在于，对于one-hot向量来说，没有自然的相似性概念。因此，另一种方法是学习在向量本身中编码相似性。核心思想是一个词的含义是由经常出现在其旁边的单词给出的。 技术2：机器翻译机器翻译是语言理解的经典测试。它由语言分析和语言生成组成。大型机器翻译系统具有巨大的商业用途。 1.端到端训练2.分布式表示的优势3.更好地探索上下文4.更流利的文本生成 技巧3：Dialogue和Conversations技术4：情绪分析用于情感分析的现代深度学习方法可用于形态学、语法和逻辑语义，其中最有效的是递归神经网络。顾名思义，递归神经网络开发的主要假设递归是描述语言的自然方式。递归在消歧方面很有用，有助于某些任务引用特定的短语，并且对于使用语法树结构的任务非常有效。 技术5：问答系统问答（QA）系统的想法是直接从文档、对话、在线搜索和其他地方提取信息，以满足用户的信息需求。QA系统不是让用户阅读整个文档，而是更喜欢简短而简洁的答案。如今，QA系统可以非常容易地与其他NLP系统结合使用，并且一些QA系统甚至超越了对文本文档的搜索，并且可以从图片集合中提取信息。 强大的深度学习架构（称为动态内存网络（DMN））已针对QA问题进行了专门开发和优化。给定输入序列（知识）和问题的训练集，它可以形成情节记忆，并使用它们来产生相关答案。该体系结构具有以下组件： ·语义内存模块（类似于知识库）被用来创建从输入句子的嵌入字序列预先训练手套载体。 ·输入模块处理与问题有关的输入矢量称为事实。该模块使用门控循环单元实现，GRU使网络能够了解当前正在考虑的句子是否相关或与答案无关。 ·问题模块逐字处理疑问词，并且使用输出相同权重的GRU输入模块的向量。事实和问题都被编码为嵌入。 ·情景记忆模块接收从输入中提取和编码的嵌入事实和问题载体。这使用了一个受大脑海马体启发的想法，它可以检索由某些反应触发的时间状态，如景点或声音。 ·答案生成模块，通过适当的响应，情景记忆应该包含回答问题所需的所有信息。该模块使用另一个GRU，使用正确序列的交叉熵错误分类进行训练，然后可以将其转换回自然语言。 技术6：文本摘要人类很难手动汇总大型文本文档。文本摘要是NLP为源文档创建简短、准确和流畅的摘要问题。随着推送通知和文章摘要获得越来越多的注意力，为长文本生成智能且准确摘要的任务每天都在增长。 文本摘要有两种基本方法：提取和抽象。前者从原始文本中提取单词和单词短语以创建摘要。后者是学习内部语言表示以生成更像人类的摘要，解释原始文本的意图。","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"},{"name":"NLP","slug":"机器学习/NLP","permalink":"http://wiki.quartz.ren/categories/机器学习/NLP/"}]},{"title":"KNN","date":"2018-12-02T03:55:57.000Z","path":"wiki/机器学习/2.KNN/","text":"概述简单的说,K-近邻算法采用测量不同特征值之间的距离方法进行分类. 工作原理: 存在一个样本数据集合, 也称作训练样本集, 并且样本集中的每个数据都存在标签(即我们知道样本集中每一数据与所属分类的对应关系),输入没有标签的新数据后.将新数据的每个特征与样本集中数据对应的特征进行比较,然后算法提取样本集中特征最相似数据(最近邻)的分类标签. 特性: 优点: 精度高,对异常值不敏感,无数据输入假设 缺点,计算复杂度高,空间复杂度高 使用数据范围: 数值型和标称型. 举例使用KNN 分类爱情片和动作片.基于电影中出现的亲吻,打斗出现的次数,使用k-近邻构造程序. 训练样本集: A 打斗镜头: 3 接吻镜头: 104 电影类型 : 爱情片B 打斗镜头: 2 接吻镜头: 100 电影类型 : 爱情片C 打斗镜头: 1 接吻镜头: 81 电影类型 : 爱情片D 打斗镜头: 101 接吻镜头: 10 电影类型 : 动作片E 打斗镜头: 99 接吻镜头: 5 电影类型 : 动作片F 打斗镜头: 98 接吻镜头: 2 电影类型 : 动作片G 打斗镜头: 18 接吻镜头: 90 电影类型 : ? 已知类型电影 (A,B,C,D,E,F) 与 未知类型电影(G) 的距离如下. 距离怎么定义的?? (二维空间的绝对距离?) A 与 G 的距离 : 20.5B 与 G 的距离 : 18.7C 与 G 的距离 : 19.2D 与 G 的距离 : 115.3E 与 G 的距离 : 117.4F 与 G 的距离 : 118.9 现在得到了样本集中所有电影与未知电影的距离,按照距离递增排序, 可以找到k个距离最近的电影.假设k=3, 则最近的三个电影依次为: F,E,D. 而 这三个都是爱情片, 因此我们判断未知电影为爱情片. 算法实现 收集数据 准备数据: 距离计算所需要的数值,最好是结构化的数据格式 分析数据 训练算法: k近邻不适用 测试算法: 计算错误率 使用算法 这里只介绍最后的算法使用步骤:实施KNN分类算法. 主要函数功能为: 使用k-近邻算法将每组数据划分到某个类别中.步骤如下 123456对未知类别属性的数据集中的每个点依次执行以下操作:1. 计算已知类别数据集中的点与当前点之间的距离2. 按照距离递增次序排序3. 选取与当前点距离最小的k个点4. 确定前k个点所在类别的出现频率 5. 返回前k个点出现频率最高的类别作为当前点的预测分类. python函数如下: 1234567891011121314def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] // 距离计算 diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) disstances = sqDistances.argsort() classCount=&#123;&#125; // 选择距离最小的k个点 for i in rang(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 其它代码见:https://github.com/quantumcs/Machine-Learning-In-Action 算法测试有关数据收集 文本数据的转换及处理etc. 其它的一些应用 约会网站的配对 手写识别系统 问题1. 一个简单的模型,存在的问题现有一个模型, 针对每个新的句子,使用相似度算出与已知样本集中每个句子的相似度. 而已知样本集只有一个类别(也就是说都是负样本). 取最大的相似度值,和阈值比较,大于阈值的定义同一个类别,小于阈值的定为不同类别. 存在的问题: 1. 没有考虑正样本,这个策略效果肯定存在提升. 2. 另外的提升思路: 对句子做预处理,去除变化较大的实体,降低对相似性的影响.","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"AdaBoost算法","date":"2018-12-02T03:55:57.000Z","path":"wiki/机器学习/4.利用AdaBoost元算法提高分类性能/","text":"组合相似的分类器来提高分类性能 应用AdaBoost算法 处理非均衡分类问题 概述元算法: 对其他算法进行组合的一种方式. 代表-AdaBoost 不同分类器的集成方法: boosting方法(代表-AdaBoost) 非均衡分类问题 基于数据集多重抽样的分类器学习了常见的分类算法:KNN,决策树,朴素贝叶斯,logistic回归.他们各有优缺点,可以将不同的分类器进行组合,而这种组合结果就被称为集成方法或者元算法. 1. bagging基于数据随机重抽样的分类器构建方法 是从原始数据集选择S次后得到S个数据集的一种技术,新数据集和原数据集的大小相等. 一种先进的bagging方法 - 随机森林 2. boosting类似bagging技术. 不管是boosting还是bagging,所使用的多个分类器的类型都是一致的,但是在前者当中,不同分类器是通过串行训练而获得的,每个新分类器都根据已训练出的分类器的性能进行训练. boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器. 由于boosting分类的结果是基于所有分类器的加权求和结果的,因此boosting与bagging不太一样. bagging中的分类器权重是相等的,而boosting中的分类器权重并不相等,每个权重代表的是其对应分类器在上一轮迭代中的成功度. boosting方法拥有多个版本,本文只关注一个最流行的版本AdaBoost. AdaBoost一般流程 1234561. 收集数据2. 准备数据: 依赖于所使用的弱分类器类型,比如单层决策树,这种分类器可以处理任何数据类型.(作为弱分类器,简单分类器的效果更好)3. 分析数据4. 训练算法: AdaBoost的大部分时间都用在训练上,分类器将多次在同一数据集上训练弱分类器5. 测试算法: 计算分类的错误率6. 使用算法:同SVM一样,AdaBoost预测两个类别中的一个.如果想把它应用到多个类别的场合,那么就要像多累SVM中的做法一样对AdaBoost进行修改 关于分类性能度量指标:正确率\\召回率及ROC曲线. 128p. 基于代价函数的分类器决策控制. p131 处理非均衡问题的数据抽样方法 p132 代码:https://github.com/quantumcs/Machine-Learning-In-Action/tree/master/Ch07","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"sbt了解","date":"2018-12-02T03:55:57.000Z","path":"wiki/工具组件/Builder/sbt/","text":"下载安装:https://www.scala-sbt.org/download.html sbt是类似ANT、MAVEN的构建工具，全称为Simple build tool，是Scala事实上的标准构建工具。 主要特性: 原生支持编译Scala代码和与诸多Scala测试框架进行交互； 使用Scala编写的DSL（领域特定语言）构建描述 使用Ivy作为库管理工具 持续编译、测试和部署 整合scala解释器快速迭代和调试 支持Java与Scala混合的项目 加速 SBT 下载依赖库的速度 sbt介绍与构建Scala项目","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Builder","slug":"工具组件/Builder","permalink":"http://wiki.quartz.ren/categories/工具组件/Builder/"}]},{"title":"数据处理相关","date":"2018-12-02T03:55:57.000Z","path":"wiki/数据处理/技术文章/Start/","text":"https://www.iteblog.com/archives/1947.html","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"技术文章","slug":"数据处理/技术文章","permalink":"http://wiki.quartz.ren/categories/数据处理/技术文章/"}]},{"title":"Python学习","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Python/Python语法学习笔记/","text":"语言比较 有非常完善的基础代码库 定位:“优雅”、“明确”、“简单” Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。如果一个资深程序员向你炫耀他写的晦涩难懂、动不动就几万行的代码，你可以尽情地嘲笑他。 可以做什么? 网站、后台服务等都可以 数据类型在内存中存储的数据可以有多种类型。 例如，一个人的年龄可以用数字来存储，他的名字可以用字符来存储。 Python 定义了一些标准类型，用于存储各种类型的数据。 Python有五个标准的数据类型： Numbers（数字） String（字符串） List（列表） Tuple（元组） Dictionary（字典） 基本类型及Python数据类型转换 1. List1list = [ 'runoob', 786 , 2.23, 'john', 70.2 ] 列表可以完成大多数集合类的数据结构实现。它支持字符，数字，字符串甚至可以包含列表（即嵌套）。列表用 [ ] 标识，是 python 最通用的复合数据类型。列表中值的切割也可以用到变量 [头下标:尾下标] ，就可以截取相应的列表，从左到右索引默认 0 开始，从右到左索引默认 -1 开始，下标可以为空表示取到头或尾。 2. Tuple1tuple = ( 'runoob', 786 , 2.23, 'john', 70.2 ) 元组是另一个数据类型，类似于List（列表）。 元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表。 3. Dictionary1234dict = &#123;&#125;dict['one'] = \"This is one\"dict[2] = \"This is two\"tinydict = &#123;'name': 'john','code':6734, 'dept': 'sales'&#125; 4. 数据类型转换Python数据类型转换 其他Python 内置函数 Python GUI编程(Tkinter) Python JSON","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"Scala学习","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Scala/Scala入门/","text":"有了java,为什么要用scala.想学scala. 先了解下它有什么比java更优秀的地方. Scala是2001年诞生的一门多范式语言 .设计初衷是要集成面向对象编程和函数式编程的各种特性 Scala 特性 面向对象特性 函数式编程 静态类型 扩展性 并发","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Scala","slug":"程序语言/Scala","permalink":"http://wiki.quartz.ren/categories/程序语言/Scala/"}]},{"title":"Scala项目构建","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Scala/使用maven构建scala项目/","text":"SCALA WITH MAVEN 123mvn archetype:generate输入 groupId等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.8&lt;/version&gt; &lt;/dependency&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- This plugin compiles Scala files --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- This plugin compiles Java files --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- This plugin adds all dependencies to JAR file during 'package' command. Pay EXTRA attention to the 'mainClass' tag. You have to set name of class with entry point to program ('main' method) --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;ScalaRunner&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; scala和maven整合实践","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Scala","slug":"程序语言/Scala","permalink":"http://wiki.quartz.ren/categories/程序语言/Scala/"}]},{"title":"Kafak监控","date":"2018-12-02T03:55:57.000Z","path":"wiki/数据处理/DataStore/Kafka/Kakfa监控/","text":"推荐的监控工具kafka-offset-monitor Python实现监控123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196topics = ['heimdallr-dev']# 要监控的groupidmonitor_group_ids = ['heimdallr']# broker-listservers = 'localhost:9092'# 监控数据上报间隔 秒time_interval = 10# 历史全量数据上报间隔history_time_interval = 5 * 60# -*- coding:utf-8 -*-import timeimport sysfrom kafka.client import KafkaClientfrom kafka.protocol.commit import OffsetFetchRequest_v1, OffsetFetchResponse_v1, OffsetFetchRequest_v0, \\ OffsetFetchResponse_v0from kafka.protocol.offset import OffsetRequest_v0, OffsetResponse_v0#from monitor_constants import *duration = 0client = Noneconn = Nonepartition_cache = &#123;&#125;brokers_cache = []kafka_type = ['heimdallr']zk_type = []def get_brokers(): if not brokers_cache: brokers = client.cluster.brokers() if brokers: brokers_cache.extend([x.nodeId for x in brokers]) return brokers_cachedef get_partitions(topic): if not partition_cache or topic not in partition_cache: partitions = client.cluster.available_partitions_for_topic(topic) if partitions: partition_cache[topic] = [x for x in partitions] else: return [] return partition_cache[topic]def get_logsize(): \"\"\" 获取topic 下每个partition的logsize(各个broker的累加) :return: \"\"\" tp = &#123;&#125; # topic : partition_dict brokers = get_brokers() for topic in topics: partitions = get_partitions(topic) pl = &#123;&#125; # partition : logsize for broker in brokers: # 这里取笛卡尔积可能有问题,但是不影响parse中解析了 for partition in partitions: client.send(broker, OffsetRequest_v0(replica_id=-1, topics=[(topic, [(partition, -1, 1)])])) responses = client.poll() pdict = parse_logsize(topic, partition, responses) append(pl, pdict) tp[topic] = pl return tpdef append(rdict, pdict): if rdict: # 已经有记录,累加 for k, v in pdict.items(): if k in rdict: rdict[k] = rdict[k] + v else: rdict[k] = v else: rdict.update(pdict)def parse_logsize(t, p, responses): \"\"\" 单个broker中单个partition的logsize :param responses: :param p: :param t: :return: \"\"\" for response in responses: if not isinstance(response, OffsetResponse_v0): return &#123;&#125; tps = response.topics topic = tps[0][0] partition_list = tps[0][1] partition = partition_list[0][0] # 异步poll来的数据可能不准 if topic == t and partition == p and partition_list[0][1] == 0: logsize_list = partition_list[0][2] logsize = logsize_list[0] return &#123;partition: logsize&#125; return &#123;&#125;def parse_offsets(t, responses): dr = &#123;&#125; for response in responses: if not isinstance(response, (OffsetFetchResponse_v1, OffsetFetchResponse_v0)): return &#123;&#125; tps = response.topics topic = tps[0][0] partition_list = tps[0][1] if topic == t: for partition_tunple in partition_list: if partition_tunple[3] == 0: offset = partition_tunple[1] dr[partition_tunple[0]] = offset return drdef get_offsets(): # &#123;gid: dict&#125; gd = &#123;&#125; for gid in monitor_group_ids: td = &#123;&#125; # &#123;topic:dict&#125; for topic in topics: pd = &#123;&#125; # &#123;partition:dict&#125; for broker in get_brokers(): partitions = get_partitions(topic) if not partitions: return &#123;&#125; else: responses = optionnal_send(broker, gid, topic, partitions) dr = parse_offsets(topic, responses) append(pd, dr) td[topic] = pd gd[gid] = td return gddef optionnal_send(broker, gid, topic, partitions): if gid in kafka_type: return kafka_send(broker, gid, topic, partitions) elif gid in zk_type: return zk_send(broker, gid, topic, partitions) else: responses = zk_send(broker, gid, topic, partitions) dct = parse_offsets(topic, responses) if is_suitable(dct): zk_type.append(gid) return responses responses = kafka_send(broker, gid, topic, partitions) dct = parse_offsets(topic, responses) if is_suitable(dct): kafka_type.append(gid) return responsesdef is_suitable(dct): for x in dct.values(): if x != -1: return Truedef kafka_send(broker, gid, topic, partitions): client.send(broker, OffsetFetchRequest_v1(consumer_group=gid, topics=[(topic, partitions)])) return client.poll()def zk_send(broker, gid, topic, partitions): client.send(broker, OffsetFetchRequest_v0(consumer_group=gid, topics=[(topic, partitions)])) return client.poll()def do_task(): offset_dict = get_offsets() #print (offset_dict) logsize_dict = get_logsize() #print (logsize_dict) print ('----------kafka monitor, info:-------------') for gk, gv in offset_dict.items(): for tk, tv in gv.items(): for pk, pv in tv.items(): if logsize_dict and tk in logsize_dict: dr = logsize_dict[tk] # partition:logsize if dr and pk in dr: param = (gk, tk, pk, pv, dr[pk], time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))) print(param)if __name__ == \"__main__\": client = KafkaClient(bootstrap_servers=servers, request_timeout_ms=3000) while True: do_task() time.sleep(time_interval) duration += time_interval 参考:https://my.oschina.net/ktlb/blog/863308","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"关于kafka的offset存储","date":"2018-11-24T17:36:57.000Z","path":"wiki/数据处理/DataStore/Kafka/关于kafka的offset存储/","text":"消费者通过offset控制消费的进度,这里有几个概念先解释一下. Offset: 每个ConsumerGroup中针对一个topic的每个Partition的消费进度.通过这个来控制消费进度. LogSize: Kafka的数据位置,随着新的数据到来而增加. Lag: LogSize - Offset . 指落后的大小. 因此正常Consumer的不堆积是Lag的值处于比较小的范围,比如 0~1000. 然而,存在的一些问题: 那随着数据量的增加,offset和logSize的值一直增加,到超过int的范围吗,还是有清零的规则.(应该是有相应的机制,这个不重要了) 有关offset的一些注意点如下 存储位置从kafka-0.9版本及以后,消费者组和offset信息就不存在zk中了,而是存到broker服务器上.存放在一个叫__consumer_offsets的topic中. 关于offset的消费者参数auto.offset.reset 123456earliest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常 也就是说,这个参数的指定只有在新的consumer group添加的时候,或者其他原因导致分区上的offset没有了的情况,才更有意义. 那随之又有的问题: 如果为了能消费新的数据,而对于老的customer-group,不想消费堆积的数据. 或者说想废弃掉这个group了,那不用之后会有什么影响 另外,对于无止尽的customer-group创建,对kafka集群有什么影响吗,当然不仅仅是新group替代旧的group.而是还有在用group的增多,会对集群有什么影响? 下面详细总结下 1. 废弃group的增多个人理解: group增多,增加了对group的管理成本,那对于不用的group,存放在broker中,不会对其它造成影响.目前只是猜测,具体再详细研究. 2. 在用group的增多对于老版本(zk管理customer信息和offset), 会增加customer与zk的交互成本. 新版本(大于0.9), customer信息和offset由broker管理,只是增加了customer与broker的交互, 然而这一部分交互信息对于整个数据流来说微乎其微,所以影响应该不大. 需要在研究下offset更新的流程(customer与broker) 再聊聊kafka的group coordinator Coordinator一般指的是运行在broker上的group Coordinator，用于管理Consumer Group中各个成员，每个KafkaServer都有一个GroupCoordinator实例，管理多个消费者组，主要用于offset位移管理和Consumer Rebalance。 在 Server 端增加了 GroupCoordinator 这个角色 将 topic 的 offset 信息由之前存储在 zookeeper(/consumers/&lt;group.id&gt;/offsets//,zk写操作性能不高) 上改为存储到一个特殊的 topic 中（__consumer_offsets） 1. rebalance时机 有新的consumer加入 旧的consumer挂了 coordinator挂了，集群选举出新的coordinator topic的partition新加 consumer调用unsubscrible()，取消topic的订阅 关于offset的提交,管理 2. __consumer_offsetsConsumer通过发送OffsetCommitRequest请求到指定broker（偏移量管理者）提交偏移量。 这个请求中包含一系列分区以及在这些分区中的消费位置（偏移量） 偏移量管理者会追加键值（key－value）形式的消息到一个指定的topic（__consumer_offsets）。key是由consumerGroup-topic-partition组成的，而value是偏移量。 感觉其实用HashMap应该更好一些,因为通过key来获取或管理offset(偏移量-value) 因为这种存储方式(队列), find的时间复杂度为O(n), 需要遍历整个__consumer_offsets,扫描全部偏移量topic日志. 因此集群的内存中也是维护了一份最近的记录,为了能在指定key的情况下能够快速的给出OffsetFetchRequests而不用扫描全部偏移量topic日志. 如果偏移量管理者因某种原因失败，新的broker将会成为偏移量管理者并且通过扫描偏移量topic来重新生成偏移量缓存。 ps: 内存中应该是Map结构,那内存中的记录与偏移量topic(__consumer_offsets)的数据怎么保证一致性的呢?? 3. Consumer与Consumer Groupconsumer group是kafka提供的可扩展且具有容错性的消费者机制。组内可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。 consumer instance可以是一个进程，也可以是一个线程. 有关offset的几个概念Kafka 之 Group 状态变化分析及 Rebalance 过程","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"Kafak环境搭建","date":"2018-11-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Kafka/Kafka环境搭建/","text":"下载kafkahttp://kafka.apache.org/quickstart 启动zookeeperbin/zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; zookeeper.out 2&gt;&amp;1 &amp; 启动kafka单机bin/kafka-server-start.sh config/server.properties &gt;&gt;kafka.out 2&gt;&amp;1 &amp; 集群启动bin/kafka-server-start.sh config/server.properties &gt;&gt;kafka-0.out 2&gt;&amp;1 &amp;bin/kafka-server-start.sh config/server-1.properties &gt;&gt;kafka-1.out 2&gt;&amp;1 &amp;bin/kafka-server-start.sh config/server-2.properties &gt;&gt;kafka-2.out 2&gt;&amp;1 &amp; 监控Kafka三款监控工具比较 Kafka监控工具KafkaOffsetMonitor配置及使用 下载KakfaOffsetMonitor 12345java -Xms512M -Xmx512M -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -cp KafkaOffsetMonitor-assembly-0.2.0.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb \\--port 8088 \\--zk 10.0.0.50:12181,10.0.0.60:12181,10.0.0.70:12181 \\--refresh 5.minutes \\--retain 1.day &gt;/dev/null 2&gt;&amp;1; TODO 监控原理 PS: kafka 日志默认保存7天. topic创建1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 5 --topic my-replicated-topic 命令行消费者1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic nginx_log --from-beginning ubuntu命令行设置系统代理 增加partition1bin/kafka-topics.sh --alter --zookeeper 127.0.0.1:2181 --partitions 10 --topic nginx_log 查看某个topic的 logSize指的是topic各个分区的logSize 1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --topic nginx_log --time -1 time 为-2 表示查看offset的最小值, -1 表示最大值1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 -topic nginx_log --time -2 查看消费者组内的offset位置(消费情况)关于kafka更改消费者对应分组下的offset值 12# To view offsets, as mentioned earlier, we &quot;describe&quot; the consumer group like this:bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --group consumer02 --describe Managing Consumer GroupsManaging Consumer Groups 123456# bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --list# provides the list of all active members in the consumer group.bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --describe --group consumer02 --members# bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --describe --group consumer02 --state 更改offset1234# 先查看一下customer的offset状态bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --group consumer02 --describe# reset offsets of a consumer group to the latest offset: (earliest)bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --reset-offsets --group consumer02 --topic nginx_log --to-latest 以上reset 只能在 consumer inactive状态时,才可以. 问题: 这个操作的目的和结果是什么??? Kafka auto.offset.reset值详解 123456earliest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"pip 模块安装","date":"2018-11-17T09:10:49.000Z","path":"wiki/程序语言/Python/模块安装/","text":"pip安装1apt install python3-pip 安装完pip就可以使用它来安装需要的模块。也可以指定安装目录1pip install xgboost --target=/home/work/.local/lib/python2.7/site-packages/ -i https://mirrors.aliyun.com/pypi/simple/","tags":[{"name":"Python","slug":"Python","permalink":"http://wiki.quartz.ren/tags/Python/"}],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"Docker相关","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Docker/8.Docker其他/","text":"Deepin下安装Docker 阿里云.来自云端的容器镜像服务 Docker镜像加速：阿里云 Docker常用操作命令 查看日志123456docker logs -f -t --since=&quot;2017-05-31&quot; --tail=10 edu_web_1--since : 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。-f : 查看实时日志-t : 查看日志产生的日期-tail=10 : 查看最后的10条日志。edu_web_1 : 容器名称 Docker 容器自定义 hosts 网络访问Docker自定义hosts网络访问 需求：使用自己的域名服务。 在很多应用环境中都会有其他服务器的访问需求。直接使用ip不便于修改。因此搭建自己的域名服务，之后配置docker环境使用此域名服务即可。也可同时使用其它域名服务。 12345(1)resolv-file=/etc/resolv.conf(2)strict-order(3)listen-address=192.168.0.127,127.0.0.1(4)addn-hosts=/etc/hosts //这歌地址指向的是你mac的hosts地址，你只需在里边做相应的 host(5)cache-size=2048 Docker镜像管理 搭建Docker私有仓库 更改Docker环境的配置指向私有仓库 向私有仓库提交镜像 从另外的docker环境获取上述镜像 Docker的镜像归纳为两种 需要知道docker容器的地址的类似zk这种组件的 不需要知道docker容器地址类似应用程序通过注册服务到zk，然后自动发现服务。 docker查看运行容器ip1docker inspect 容器ID | grep IPAddress","tags":[{"name":"Docker","slug":"Docker","permalink":"http://wiki.quartz.ren/tags/Docker/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"ES环境搭建","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Elasticsearch/1.ES环境搭建/","text":"Elasticsearch Reference 搭建集群环境下载安装包安装步骤 下载并解压 修改配置文件 default ： $ES_HOME/config/elasticsearch.yml 一版需要修改的几个地方如下： 1234567891011121314# 集群名称cluster.name: my-application# 节点名称node.name: node-1# 绑定的IPnetwork.host: 192.168.1.106# 开始发现新节点的IPdiscovery.zen.ping.unicast.hosts: [&quot;192.168.1.108&quot;, &quot;192.168.1.106&quot;]# 最多发现主节点的个数discovery.zen.minimum_master_nodes: 1# 当重启集群节点后最少启动N个节点后开始做恢复gateway.recover_after_nodes: 3# 在一台机器上最多启动的节点数node.max_local_storage_nodes: 1 具体配置文件解释见: elasticsearch配置文件详解 配置集群模式，在其它节点修改以上对应配置项，启动即可。 启动1./bin/elasticsearch -d","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://wiki.quartz.ren/tags/elasticsearch/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Elasticsearch","slug":"工具组件/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/工具组件/Elasticsearch/"}]},{"title":"Python客户端创建索引","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Elasticsearch/Python客户端创建索引/","text":"安装elasticsearch模块 进入python交互界面 引入es的模块 1from elasticsearch import Elasticsearch 定义es链接及变量 12doc_type = &apos;log&apos;es = Elasticsearch(urls = [&apos;http://192.168.1.108:9200&apos;,&apos;http://192.168.1.106:9200&apos;], timeout = 60, max_retries = 0) 设置mapping 创建索引1es.index(index = &apos;app-log&apos;, doc_type=&apos;log&apos;, body = mapping) 12345678910111213141516171819202122232425262728293031323334mapping = &#123; &apos;settings&apos;: &#123; &apos;index&apos;: &#123; &apos;number_of_replicas&apos;: 1, &apos;number_of_shards&apos;: 6, &apos;refresh_interval&apos;: &apos;5s&apos; &#125; &#125;, &apos;mappings&apos;: &#123; &apos;_default_&apos;: &#123; &apos;_all&apos;: &#123; &apos;enabled&apos;: False &#125; &#125;, doc_type : &#123; &apos;properties&apos; : &#123; &apos;day&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;time&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;nanoTime&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;created&apos;: &#123; &apos;type&apos;: &apos;date&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;app&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;host&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;thread&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;level&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;eventType&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;pack&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;clazz&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;line&apos;: &#123; &apos;type&apos;: &apos;string&apos;, &apos;index&apos;: &apos;not_analyzed&apos;&#125;, &apos;messageSmart&apos;: &#123; &apos;type&apos;: &apos;text&apos;, &apos;analyzer&apos;: &apos;ik_smart&apos;, &apos;search_analyzer&apos;: &apos;ik_smart&apos;, &apos;include_in_all&apos;: &apos;true&apos;, &apos;boost&apos;: 8&#125;, &apos;messageMax&apos;: &#123; &apos;type&apos;: &apos;text&apos;, &apos;analyzer&apos;: &apos;ik_max_word&apos;, &apos;search_analyzer&apos;: &apos;ik_max_word&apos;, &apos;include_in_all&apos;: &apos;true&apos;, &apos;boost&apos;: 8&#125; &#125; &#125; &#125; &#125; 验证1curl &apos;192.168.1.108:9200/_cat/indices?v&apos; 完整脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/usr/bin/python# -*- coding: UTF-8 -*-import sysimport datetimefrom pyelasticsearch import ElasticSearchfrom pyelasticsearch import bulk_chunksdef main(argv): doc_type = 'log' index = 'app-log' es = ElasticSearch(urls = ['http://192.168.1.108:9200'], timeout = 60, max_retries = 0) mapping = &#123; 'settings': &#123; 'index': &#123; 'number_of_replicas': 1, 'number_of_shards': 6, 'refresh_interval': '5s' &#125; &#125;, 'mappings': &#123; '_default_': &#123; '_all': &#123; 'enabled': False &#125; &#125;, doc_type : &#123; 'properties' : &#123; 'day': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'time': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'nanoTime': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'created': &#123; 'type': 'date', 'index': 'not_analyzed'&#125;, 'app': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'host': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'thread': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'level': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'eventType': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'pack': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'clazz': &#123; 'type': 'string', 'index': 'not_analyzed'&#125;, 'line': &#123; 'type': 'string', 'index': 'not_analyzed'&#125; # 'messageSmart': &#123; 'type': 'string', 'analyzer': 'ik_smart', 'search_analyzer': 'ik_smart', 'include_in_all': 'true', 'boost': 8&#125;, &#125; &#125; &#125; &#125; es.create_index(index = index, settings = mapping)if __name__ == '__main__': main(sys.argv)","tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"http://wiki.quartz.ren/tags/elasticsearch/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Elasticsearch","slug":"工具组件/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/工具组件/Elasticsearch/"}]},{"title":"Jenkins安装配置","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Jenkins/2.Jenkins安装配置/","text":"1.下载war文件。 下载地址. 2.启动 启动脚本如下： 12#set JENKINS_HOME=nohup java -jar jenkins* --httpPort=8000 --prefix=/jks &gt;&gt; nohup.out 2&gt;&amp;1 &amp; 默认JENKINS_HOME为 .jenkins。默认端口8080 3.验证 在浏览器输入：http://127.0.0.1:8000 4.输入安装密码，进入初始化页面 在启动之后，日志中会出现一个安装密码。打开页面时需要配置。 选择推荐插件模式进行安装。（等5分钟） 5.创建第一个管理员账户（请牢记密码） 6.安装成功","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://wiki.quartz.ren/tags/Jenkins/"},{"name":"CI","slug":"CI","permalink":"http://wiki.quartz.ren/tags/CI/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Jenkins","slug":"工具组件/Jenkins","permalink":"http://wiki.quartz.ren/categories/工具组件/Jenkins/"}]},{"title":"Jenkins认识","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Jenkins/1.Jenkins认识/","text":"Jenkins可以做什么？软件开发实践过程中有很多重复性的工作，并且团队成员都需要经常集成他们的工作。 持续集成：一种软件开发实践。通过自动化的构建（包括编译，发布，自动化测试)来验证集成的工作任务。可以尽快的发现集成错误，可以大大减少集成的问题，让团队能够更快的开发内聚的软件。 而Jenkins的定义是一个可扩展的持续集成引擎。 持续集成不仅仅是自动化部署，它的重要性还包含了项目质量的提高以及减少重复的操作等等。 怎么让持续集成的价值体现出来呢？首先从这个持续集成工具开始。 持续集成的要素 1. Jenkins可以做的事最基本的就是项目构建，再就是通过Jenkins的插件围绕构建的一些工作进行自动化。比如以下： 自动化项目构建 自动化测试 自动化部署 Jenkins的强大依赖于很多第三方插件。 2. Jenkins实用插件 iOS专用：Xcode integration Android专用：Gradle plugin Gitlab插件：GitLab Plugin 和 Gitlab Hook Plugin Git插件： Git plugin GitBuckit插件： GitBuckit plugin 签名证书管理插件: Credentials Plugin 和Keychains and Provisioning Profiles Management FTP插件: Publish over FTP 脚本插件: Post-Build Script Plug-in 修改Build名称/描述(二维码)： build-name-setter / description setter plugin 获取仓库提交的commit log： Git Changelog Plugin 自定义全局变量: Environment Injector Plugin 自定义邮件插件： Email Extension Plugin 获取当前登录用户信息： build-user-vars-plugin 显示代码测试覆盖率报表： Cobertura Plugin 来展示生成的单元测试报表，支持一切单测框架，如junit、nosetests等： Junit Plugin 其它： GIT plugin / SSH Credentials Plugin 3. 基础环境配置 maven jdk 参考以下Jenkins Gitlab持续集成打包平台搭建 Jenkins使用 Jenkins系列文章","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://wiki.quartz.ren/tags/Jenkins/"},{"name":"CI","slug":"CI","permalink":"http://wiki.quartz.ren/tags/CI/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Jenkins","slug":"工具组件/Jenkins","permalink":"http://wiki.quartz.ren/categories/工具组件/Jenkins/"}]},{"title":"Jenkins插件使用","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Jenkins/3.Jenkins插件使用/","text":"Version Number Plug-In版本管理：自动生成工单版本（例如201712280。最后一位0代表当天第几次构建）。记录历史构建版本。版本规则可自定义。 Role-based Authorization Strategy项目权限管理：用户只能查看、修改、构建自己相关项目 参考：Jenkins权限控制 这里的项目权限控制，通过项目名称的正则匹配，并没有直接或间接的关联（像通常的管理系统权限管理）。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://wiki.quartz.ren/tags/Jenkins/"},{"name":"CI","slug":"CI","permalink":"http://wiki.quartz.ren/tags/CI/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Jenkins","slug":"工具组件/Jenkins","permalink":"http://wiki.quartz.ren/categories/工具组件/Jenkins/"}]},{"title":"Jenkins项目常用配置","date":"2018-11-16T17:13:34.000Z","path":"wiki/工具组件/Jenkins/4.Jenkins项目常用配置/","text":"主要是maven项目的构建 不是父子关系的依赖项目，关联构建 父子关系项目 表示子模块，通过pom.xml中模块关联。两个项目之间互为父子项目。配置如下： 12345678910&lt;modules&gt; &lt;module&gt;aaa&lt;/module&gt; &lt;module&gt;bbb&lt;/module&gt; &lt;module&gt;ccc&lt;/module&gt;&lt;/modules&gt;&lt;parent&gt; &lt;groupId&gt;com.xxx&lt;/groupId&gt; &lt;artifactId&gt;cc&lt;/artifactId&gt; &lt;version&gt;0001&lt;/version&gt;&lt;/parent&gt; 依赖项目关联构建 不属于上述情况，但项目之间是依赖关系。所以在构建需要的项目时，希望依赖的项目是最新的构建。 对于这种情况，就需要一种jenkins构建方案。目前发现的一种配置方式如下，可供参考。 配置项目如下： 表示：在构建当前项目时，会阻塞去构建依赖的项目，之后再完成当前项目的构建。","tags":[{"name":"Jenkins","slug":"Jenkins","permalink":"http://wiki.quartz.ren/tags/Jenkins/"},{"name":"CI","slug":"CI","permalink":"http://wiki.quartz.ren/tags/CI/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Jenkins","slug":"工具组件/Jenkins","permalink":"http://wiki.quartz.ren/categories/工具组件/Jenkins/"}]},{"title":"Netty学习目录","date":"2018-11-16T17:13:34.000Z","path":"wiki/MicroService/技术框架/Netty/Netty学习目录/","text":"Java NIO unix网络IO模型 Java IO的演进 四种IO的概念与比较 Netty NIO开发 服务端、客户端Demo Tcp粘包、拆包问题 分隔符和定长解码器的应用 Netty编解码开发 编解码技术（序列化、Protobuf、Thrift、Marshaling） MessagePack编解码 Google Protobuf编解码 JBoss Marshaling编解码 Netty多协议开发 Http协议开发 WebSoket协议开发 私有协议栈开发 私有协议介绍 协议栈功能设计 协议栈功能开发 服务端创建细节客户端创建细节 源码分析 ByteBuf Channel和Unsafe ChannelPipeline和ChannelHandler EventLoop和EventLoopGroup NioEventLoop源码分析 Future和Promise 架构和行业应用 Java NIOunix网络IO模型Java IO的演进四种IO的概念与比较Netty NIO开发服务端、客户端DemoTcp粘包、拆包问题分隔符和定长解码器的应用Netty编解码开发编解码技术（序列化、Protobuf、Thrift、Marshaling）MessagePack编解码Google Protobuf编解码JBoss Marshaling编解码Netty多协议开发Http协议开发WebSoket协议开发私有协议栈开发私有协议介绍协议栈功能设计协议栈功能开发服务端创建细节客户端创建细节源码分析ByteBufChannel和UnsafeChannelPipeline和ChannelHandlerEventLoop和EventLoopGroupNioEventLoop源码分析Future和Promise架构和行业应用已连接netty/README.md","tags":[{"name":"Netty","slug":"Netty","permalink":"http://wiki.quartz.ren/tags/Netty/"}],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"},{"name":"技术框架","slug":"MicroService/技术框架","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/"},{"name":"Netty","slug":"MicroService/技术框架/Netty","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/Netty/"}]},{"title":"读写事件流程分析与相关API","date":"2018-11-16T17:13:34.000Z","path":"wiki/MicroService/技术框架/Netty/读写事件流程分析与相关API/","text":"netty5笔记-总体流程分析3-ChannelHandlerContext Netty4学习笔记（1）– ChannelPipeline 处理链处理写操作流程、Pipeline，Context和Handler的协作处理。 netty核心的概念：Channel、Buffer、Selecter ChannelChannel是核心的一个接口，表示一个联络Socket的通道。通过Channel，可以对Socket进行各种操作。 ChannelHandler在实际程序实现中，很少直接操作Channel，而是通过ChannelHandler来间接操纵Channel。（使用策略模式？） ChannelHandler的种类:ChannelHandler接口的子接口 ChannelInboundHandler ChannelOutboundHandler 5.0.0.Alpha1 版本已经没有了这两个接口，why。4.1.5存在。 ChannelPipelineChannelPipeline里有一个双向链表，使用HashMap存放节点。节点类型为：ChannelHandlerContext。 是一个ChandlerHandler的链表。当需要对Channel进行某种处理的时候，Pipeline负责依次调用每一个Handler进行处理。（责任链模式？） 每个Channel都有一个属于自己的Pipeline.(一对一关系) ChannelPipeline的方法有很多，其中一部分是用来管理ChannelHandler的，如下面这些： 12345678910111213ChannelPipeline addFirst(String name, ChannelHandler handler); ChannelPipeline addLast(String name, ChannelHandler handler); ChannelPipeline addBefore(String baseName, String name, ChannelHandler handler); ChannelPipeline addAfter(String baseName, String name, ChannelHandler handler); ChannelPipeline remove(ChannelHandler handler); ChannelHandler remove(String name); ChannelHandler removeFirst(); ChannelHandler removeLast(); ChannelPipeline replace(ChannelHandler oldHandler, String newName, ChannelHandler newHandler); ChannelHandler replace(String oldName, String newName, ChannelHandler newHandler); ChannelHandler first(); ChannelHandler last(); ChannelHandler get(String name); 事件的传播为了搞清楚事件如何在Pipeline里传播， 让我们从Channel的抽象子类AbstractChannel开始： 12345678public abstract class AbstractChannel extends DefaultAttributeMap implements Channel &#123; // ... @Override public Channel write(Object msg) &#123; return pipeline.write(msg); &#125; // ... &#125; 再看DefaultChannelPipeline的write()方法实现 12345678final class DefaultChannelPipeline implements ChannelPipeline &#123; // ... @Override public ChannelFuture write(Object msg) &#123; return tail.write(msg); &#125; // ... &#125; 因为write是个outbound事件，所以DefaultChannelPipeline直接找到tail部分的context，调用其write()方法 接着看DefaultChannelHandlerContext的write()方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546final class DefaultChannelHandlerContext extends DefaultAttributeMap implements ChannelHandlerContext &#123; // ... @Override public ChannelFuture write(Object msg) &#123; return write(msg, newPromise()); &#125; @Override public ChannelFuture write(final Object msg, final ChannelPromise promise) &#123; if (msg == null) &#123; throw new NullPointerException(&quot;msg&quot;); &#125; validatePromise(promise, true); write(msg, false, promise); return promise; &#125; private void write(Object msg, boolean flush, ChannelPromise promise) &#123; DefaultChannelHandlerContext next = findContextOutbound(); next.invokeWrite(msg, promise); if (flush) &#123; next.invokeFlush(); &#125; &#125; private DefaultChannelHandlerContext findContextOutbound() &#123; DefaultChannelHandlerContext ctx = this; do &#123; ctx = ctx.prev; &#125; while (!ctx.outbound); return ctx; &#125; private void invokeWrite(Object msg, ChannelPromise promise) &#123; try &#123; ((ChannelOutboundHandler) handler).write(this, msg, promise); &#125; catch (Throwable t) &#123; notifyOutboundHandlerException(t, promise); &#125; &#125; // ... &#125; context的write()方法沿着context链往前找，直至找到一个outbound类型的context为止，然后调用其invokeWrite()方法 invokeWrite()接着调用handler的write()方法： ChannelHandlerContextChannelPipeline并不是直接管理ChannelHandler，而是通过ChannelHandlerContext来间接管理，这一点通过ChannelPipeline的默认实现DefaultChannelPipeline可以看出来。 调用ChannelHandlerContext#channel()方法可以得到和Context绑定的Channel，调用ChannelHandlerContext#handler()方法可以得到和Context绑定的Handler。 一个ChannelHandlerContext只能对应一个ChannelHander，只对应一个Channel，而一个ChannelHander则可以对应多个ChannelHandlerContext","tags":[{"name":"Netty","slug":"Netty","permalink":"http://wiki.quartz.ren/tags/Netty/"}],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"},{"name":"技术框架","slug":"MicroService/技术框架","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/"},{"name":"Netty","slug":"MicroService/技术框架/Netty","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/Netty/"}]},{"title":"SpringBoot","date":"2018-11-16T17:13:34.000Z","path":"wiki/MicroService/技术框架/SpringBoot/Readme/","text":"Spring Boot 2.0深度实践之核心技术","tags":[{"name":"springboot","slug":"springboot","permalink":"http://wiki.quartz.ren/tags/springboot/"}],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"},{"name":"技术框架","slug":"MicroService/技术框架","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/"},{"name":"SpringBoot","slug":"MicroService/技术框架/SpringBoot","permalink":"http://wiki.quartz.ren/categories/MicroService/技术框架/SpringBoot/"}]},{"title":"Hadoop学习资料","date":"2018-02-16T04:56:24.000Z","path":"wiki/数据处理/DataStore/Hadoop/Hadoop学习资料/","text":"hadoop-notebook HDFS MapReduce Yarn HDFShdfs架构图Hadoop核心之HDFS 架构设计 数据块 NameNode DataNode SecondaryNameNode 各自的作用 MapReduceMapreduce是一个计算框架。 MapReduce讲的就是分而治之的程序处理理念。 MapReduce的根本原则是信息处理的本地化，哪台PC持有相应要处理的数据，哪台PC就负责处理该部分的数据，这样做的意义在于可以减少网络通讯负担。 学习的demo Welcome to the Hadoop installed wiki! 以下是相关资料 搭建hadoop-2.6.0-cdh5.4.7伪分布式 apache hadoop-2.6.0-CDH5.4.1 安装:完全分布式 官网：Hadoop分布式文件系统：架构和设计 Yarn 和MapReduce比较 HBase 默认配置 基于CDH5.4.5（ha）的Hbase 3节点搭建 分布式系统概述（Hadoop与HBase的前生今世） HBase介绍、搭建、环境、安装部署:架构、数据结构、原理 zk的作用及spark HBase之单机模式与伪分布式模式安装 HBase命令及数据结构//TODO HBase HMaster的作用： 为Region server分配region负责Region server的负载均衡发现失效的Region server并重新分配其上的regionHDFS上的垃圾文件回收处理schema更新请求 HRegionServer作用： 维护master分配给他的region，处理对这些region的io请求负责切分正在运行过程中变的过大的region可以看到，client访问HBase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列族创建一个Store实例，每个Store都会有一个MemStore和0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。一个HRegionServer会有多个HRegion和一个HLog。Welcome to the Hadoop installed wiki! 以下是相关资料 搭建hadoop-2.6.0-cdh5.4.7伪分布式 apache hadoop-2.6.0-CDH5.4.1 安装:完全分布式 官网：Hadoop分布式文件系统：架构和设计 Yarn 和MapReduce比较 HBase 默认配置 基于CDH5.4.5（ha）的Hbase 3节点搭建 分布式系统概述（Hadoop与HBase的前生今世） HBase介绍、搭建、环境、安装部署:架构、数据结构、原理 zk的作用及spark HBase之单机模式与伪分布式模式安装 HBase命令及数据结构//TODO HBaseHMaster的作用：为Region server分配region负责Region server的负载均衡发现失效的Region server并重新分配其上的regionHDFS上的垃圾文件回收处理schema更新请求 HRegionServer作用：维护master分配给他的region，处理对这些region的io请求负责切分正在运行过程中变的过大的region可以看到，client访问HBase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列族创建一个Store实例，每个Store都会有一个MemStore和0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。一个HRegionServer会有多个HRegion和一个HLog。 已连接data/bigdata/hadoop-learning/resources.md","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://wiki.quartz.ren/tags/hadoop/"}],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Hadoop","slug":"数据处理/DataStore/Hadoop","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Hadoop/"}]},{"title":"ES基础","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES基础/","text":"Docker安装esInstall Elasticsearch with Docker es 建议先手动安装. head 插件可以使用doker创建 12docker pull elasticsearchdocker run -d -p 9200:9200 --name=es-server elasticsearch 使用另外一个镜像做head插件 1docker run --name elasticsearch-head -d -p 9100:9100 mobz/elasticsearch-head:5 使用docker可视化界面管理容器 1docker run --name docker-ui -d -p 9000:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker ES安装下载 elasticsearch-6.3.2.tar.gz. 1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.zip 解压后:修改配置文件,添加 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 上述是由于head插件和es 之间存在跨域问题(两个进程),需要给权限 https://www.jianshu.com/p/f80fb1dd842b 12# 启动bin/elasticsearch -d 安装插件analysis-ik1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.3.2.zip https://github.com/medcl/elasticsearch-analysis-ik 安装sql插件1./bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.3.2.0/elasticsearch-sql-6.3.2.0.zip https://github.com/NLPchina/elasticsearch-sql 其他插件https://www.elastic.co/guide/en/elasticsearch/plugins/6.3/analysis.html 安装kibana1wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gz http://www.quartz.ren:5601/app/kibana#/home?_g=())","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"ES常用API","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES常用api/","text":"常用API 查看所有索引1http://yun.quartz.ren:9200/_cat/indices?v 索引和搜索文档索引里面还有类型的概念，在索引文档之前要先设置类型type 查询所有文档(索引的所有文档) 1234GET /lagou_job/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 查看索引结构其他语言的APIpython API","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"Mongo基础","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Mongo/Mongo基础/","text":"Docker安装mongo12docker pull mongodocker run --name mongo-server -p 27017:27017 -v /data/mongo/db:/data/db -d mongo 使用mongo镜像执行mongo命令了连接容器 1docker run -it mongo mongo --host 172.17.0.2 相关命令1234命令行登陆数据库（类似 mysql -uroot -p -h）mongo --port 27017use admindb.auth(&quot;adminUser&quot;, &quot;adminPass&quot;) 1234567891011&gt; 查看数据库show databases; // 或者 show dbs;&gt; 创建数据库use new_db; //只有插入数据,db才会真正创建.&gt; 创建集合后要再插入一个文档(记录)db.test.insert(&#123;&quot;name&quot;:&quot;菜鸟&quot;&#125;)&gt; 删除数据库db; // 查看当前数据库db.dropDatabase();//删除当前数据库&gt; 集合的删除db.collection.drop(); // 在当前数据库下的集合 12&gt; 查看集合show collections 集合创建集合创建 集合(collection)和table的区别? 创建固定大小的集合 12db.createCollection(&quot;mycol&quot;, &#123; capped : true, autoIndexId : true, size : 6142800, max : 10000 &#125; ) 文档文档的数据结构和JSON基本一样。所有存储在集合中的数据都是BSON格式。BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。 1234567db.col.insert(&#123;title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;) 查看已经插入的文档 1db.col.find() 也可以将数据定义为一个变量,然后插入 12345678document=(&#123;title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;);db.col.insert(document); 文档更新语法MongoDB 更新文档 123456789101112131415161718192021222324252627282930db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;)例如:db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;)db.col.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;56064f89ade2f21f36b03136&quot;), &quot;title&quot; : &quot;MongoDB&quot;, &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;, &quot;by&quot; : &quot;菜鸟教程&quot;, &quot;url&quot; : &quot;http://www.runoob.com&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;database&quot;, &quot;NoSQL&quot; ], &quot;likes&quot; : 100&#125;以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。&gt;db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;,&#123;multi:true&#125;) 删除文档MongoDB 删除文档 查询文档MongoDB 查询文档 MongoDB 条件操作符 MongoDB $type 操作符 MongoDB 索引MongoDB 索引 创建索引语法: 1db.collection.createIndex(keys, options) MongoDB JavaMongoDB Java","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Mongo","slug":"数据处理/DataStore/Mongo","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Mongo/"}]},{"title":"使用python添加数据到mongo","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Mongo/使用python添加数据到Mongo/","text":"123456789101112131415161718192021222324252627282930313233343536import pymongoimport jsonclient = pymongo.MongoClient(\"localhost\", 27017)db = client.test# print (db.name)# print (db.my_collection)# db.my_collection.insert_one(&#123;\"x\": 10&#125;).inserted_id# create collection . 可以不用创建,插入数据会自动创建## db.createCollection(\"UserBehavior\", &#123; capped : true, autoIndexId : true, size : 6142800, max : 100000 &#125; )fo = open(\"/home/zhangquanquan/workspace/stream-project/my-flink-project/src/main/resources/UserBehavior.csv\", \"r\")print (\"文件名为: \", fo.name)fileList = fo.readlines()fo.close()for line in fileList: line = line.strip() lineItem = line.split(\",\") ub = &#123;&#125; ub['userId'] = lineItem[0] ub['itemId'] = lineItem[1] ub['categoryId'] = lineItem[2] ub['behavior'] = lineItem[3] ub['timestamp'] = lineItem[4] json_ub = json.dumps(ub) db.user_behavior.insert(ub) # 批量插入,可以使用 db.collection.insertMany([&#123;\"b\": 3&#125;, &#123;'c': 4&#125;]) # print (json_ub)print (\"finished.\")# for item in db.my_collection.find().sort(\"x\", pymongo.ASCENDING):# print(item[\"x\"])","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Mongo","slug":"数据处理/DataStore/Mongo","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Mongo/"}]},{"title":"Oracle批量测试数据生成","date":"2018-01-08T03:55:57.000Z","path":"wiki/数据处理/DataStore/Oracle/Oracle批量测试数据生成/","text":"oracle使用declare 1234567891011121314declarei integer; --定义变量 begin i := 1;loop /* 插入数据: 自定义 */INSERT INTO table VALUES(i, '20171016');/* 参数递增 */i := i + 1;/* 停止条件 */ exit when i &gt; 100000; end loop;commit;end;","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Oracle","slug":"数据处理/DataStore/Oracle","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Oracle/"}]},{"title":"ActiveMQ介绍","date":"2017-11-16T17:13:34.000Z","path":"wiki/工具组件/ActiveMQ/ActiveMQ介绍/","text":"使用方式（嵌入模式、集群模式） 概念：具体某一个队列、整个队列的创建、使用。一个broker 连接方式（连接池？） 支持协议 几个端口作用、以及监控实现 消息模式（发送订阅和点对点消息） 与其它消息队列比较 消息确认机制（事物） ActiveMQ的设置消息时长，事务，确认机制 ，持久化(六)（http://blog.csdn.net/u014401141/article/details/54772847） 集群方案（内嵌代理所引发的问题：消息过载、管理混乱） 队列使用 讲解流程前言：断连导致消息丢失问题，消费者没有中断。1、队列的优点，大概说一下（异步、系统之间解耦）2、队列的使用场景，复杂的交易系统消息异步，实现系统解耦合。 3、activemq相关（几个端口，支持的协议，应用的连接方式，队列的详细：broker、及队列及管理）消息模式，消息时长的设置，事务，持久化机制。4、我们对activemq的应用，应用内嵌模式。及用了之后的好处(异步，提高系统性能、缓冲或者流量削峰，降低系统压力、防止不正常因素&lt;系统停止，连接断连，&gt;导致消息丢失，持久化消息)或者说弥合 生产者和消费者速度和稳定性 不一致的差异。实现系统间解耦。 举例，之前的积分系统，需要请求账户系统做积分增加。但账户系统某一天凌晨2点会停机升级。那这个积分就加不上了。目前系统中的断连的情况，没有消息队列，消息只能丢失，或者没有好的处理方式防止丢失。 为什么需要消息队列 系统的性能（并发量，吞吐量，响应时间）会有瓶颈 聊聊QPS/TPS/并发量/系统吞吐量的概念 短信发送时间肯定能达到1000，但完成单个事务的平均处理时间受到数据库操作的影响，两个同步数据库操作，一个insert，一个update。所以最多处理一秒1000个比较难。做一些细节的优化。 数据库优化","tags":[{"name":"activemq","slug":"activemq","permalink":"http://wiki.quartz.ren/tags/activemq/"},{"name":"mq","slug":"mq","permalink":"http://wiki.quartz.ren/tags/mq/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"ActiveMQ","slug":"工具组件/ActiveMQ","permalink":"http://wiki.quartz.ren/categories/工具组件/ActiveMQ/"}]},{"title":"ActiveMQ分享","date":"2017-11-16T17:13:34.000Z","path":"wiki/工具组件/ActiveMQ/ActiveMQ分享/","text":"队列的优点总结整理实际应用 具体使用的细节 消费者线程的问题，以及队列使用模式/流程 队列的优点、有哪些场景以及好处、其中包括我们使用队列的场景 我们使用队列的流程以及短信消息处理的流程 队列相关的概念（broker、创建一个队列） 交互的协议介绍、及各个端口的作用、连接方式（协议、连接池） 消息模式：我们使用的、还有其他模式 集群与单点 消息的流程、有哪些线程。可以知道在哪个环境出问题。 提升系统的稳定性提升响应速度","tags":[{"name":"activemq","slug":"activemq","permalink":"http://wiki.quartz.ren/tags/activemq/"},{"name":"mq","slug":"mq","permalink":"http://wiki.quartz.ren/tags/mq/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"ActiveMQ","slug":"工具组件/ActiveMQ","permalink":"http://wiki.quartz.ren/categories/工具组件/ActiveMQ/"}]},{"title":"Wiki Start","date":"2017-02-16T04:56:24.000Z","path":"wiki/WikiStart/","text":"https://notebook.zoho.com.cn https://github.com/Avik-Jain/ 环境安装 安装nodejs 环境 https://www.quartz.ren/2017/10/12/nodejs-env/ 安装hexo 123456npm install -g hexo-cli...等好长时间ln -s /opt/node-v4.4.4-linux-x86/bin/hexo /usr/local/bin/hexo## 加速方案npm install -g hexo-cli --registry=http://registry.npm.taobao.org 创建hexo项目-建站 https://hexo.io/zh-cn/docs/setup 使用Wikitten主题 https://github.com/zthxxx/hexo-theme-Wikitten 主要的命令如下: 123456git clone https://github.com/zthxxx/hexo-theme-Wikitten.git themes/Wikittencp -rf themes/Wikitten/_source/* source/cp -rf themes/Wikitten/_scaffolds/* scaffolds/cp -f themes/Wikitten/_config.yml.example themes/Wikitten/_config.yml# edit and customize itvim themes/Wikitten/_config.yml 一些其他的配置 gitee 编辑环境 不错的vpn推荐 https://wiki.zthxxx.me/wiki/index/ Hexo Quick Start## Create a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Welcome Nova’s Wiki为什么要有这个章节呢？整理这本零散的书，只是想把遇到的一些问题，技术做一个总结。积累下来。 而作为一名技术人员。计算机的基础是很重要的。很多程序员都是每天在复制粘贴，完成一些业务逻辑。高级一点在就是复制别的一些很不错的code。引用过来。 但是对于比较深的技术方案，生产问题，技术架构，基础知识在这里起着至关重要的作用。 大多数技术蜻蜓点水的了解是需要的，但更需要对某个点更深层次的学习。这样才会在一些领域站住脚跟。 虽然说计算机行业技术更新速度日新月异，但基础知识也就是那些，变动还是不会太大。 所以，要想做一些更深层次的事，就待学习一个东西到更深的层次，同时学习好基础。come on。 博客","tags":[{"name":"wiki","slug":"wiki","permalink":"http://wiki.quartz.ren/tags/wiki/"},{"name":"hexo","slug":"hexo","permalink":"http://wiki.quartz.ren/tags/hexo/"}],"categories":[]},{"title":"Git学习","date":"2015-11-16T17:13:34.000Z","path":"wiki/工具组件/IDE/Git学习/","text":"svn与git的比较 时光机穿梭 git的分支管理 git的命令 svn与git的比较集中式vs分布式分支管理版本管理（回退-某个文件、整个项目工程） 时光穿梭机Git会把所有人的提交串成一条时间线。（使用可视化工具可以清楚的看到） git reset ： 版本回退（回到过去，回到将来） git reflog ： 查看每一次命令 工作区、暂存区git的命令分为两种 一种是本地仓库相关的。另一种是提交远程仓库的","tags":[{"name":"Git","slug":"Git","permalink":"http://wiki.quartz.ren/tags/Git/"},{"name":"版本管理","slug":"版本管理","permalink":"http://wiki.quartz.ren/tags/版本管理/"}],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"IDE","slug":"工具组件/IDE","permalink":"http://wiki.quartz.ren/categories/工具组件/IDE/"}]}]}