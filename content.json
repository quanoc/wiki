{"pages":[{"title":"Categories","date":"2019-02-23T13:37:58.003Z","path":"categories/index.html","text":""},{"title":"About","date":"2019-02-23T13:37:58.002Z","path":"about/index.html","text":""},{"title":"Tags","date":"2019-02-23T13:37:58.004Z","path":"tags/index.html","text":""}],"posts":[{"title":"在看的书和文章","date":"2019-02-24T01:56:24.000Z","path":"wiki/生活学习/在看的书和文章/","text":"个人博客 Docker — 从入门到实践 学习java设计模式 设计模式之禅（第2版） hadoop-notebook gitee-pages 有趣的文章Web应用架构演进及系统性能、稳定性所需要解决的问题 dubbo学习过程、使用经验分享及实现原理简单介绍","tags":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/tags/生活学习/"}],"categories":[{"name":"生活学习","slug":"生活学习","permalink":"http://wiki.quartz.ren/categories/生活学习/"}]},{"title":"ubunut 挂载samba","date":"2019-02-24T01:56:24.000Z","path":"wiki/软件工程/Linux/ubuntu挂载samba/","text":"ubunut 挂载smbubuntu18.04挂载smblinux挂载samba文件系统的方法12sudo apt install cifs-utilssudo mount -t cifs -o username=xxx,password=xxx //$&#123;ip&#125;/deploy/download /mnt/download sslocal -s 108.160.129.222 -p 25002 -k ‘2018vultr@lgqsb’ -l 1080 -t 600 -m aes-256-cfb -d $1 –pid-file /tmp/vpn-agent.pid –log-file /tmp/vpn-agent.log export GIO_EXTRA_MODULES=/usr/lib/x86_64-linux-gnu/gio/modules/ if [ $1 == ‘stop’ ];then gsettings set org.gnome.system.proxy mode ‘none’ echo ‘stop 1232.’else gsettings set org.gnome.system.proxy.http host ‘127.0.0.1’ gsettings set org.gnome.system.proxy.http port 1080 gsettings set org.gnome.system.proxy mode ‘manual’ echo ‘start 22222.’fi","tags":[{"name":"linux","slug":"linux","permalink":"http://wiki.quartz.ren/tags/linux/"}],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"Linux","slug":"软件工程/Linux","permalink":"http://wiki.quartz.ren/categories/软件工程/Linux/"}]},{"title":"","date":"2019-02-23T13:37:58.002Z","path":"wiki/软件工程/问题/关于非技术类的几个问题/","text":"创业的流程 别人的一些经历,经验,需要学习的地方,思想 有关管理,交流的问题,人的管理.","tags":[],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"问题","slug":"软件工程/问题","permalink":"http://wiki.quartz.ren/categories/软件工程/问题/"}]},{"title":"","date":"2019-02-23T13:37:57.987Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES权限设置/","text":"参考 开启权限 1curl -H &quot;Content-Type:application/json&quot; -XPOST http://yun.quartz.ren:9200/_xpack/license/start_trial?acknowledge=true 修改es的配置 12添加xpack.security.enabled: true 设置用户名和密码详见 1bin/elasticsearch-setup-passwords interactive 重要: 在Kibana中配置Security 添加用户,角色等操作. 6.3版本x-pack破解","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"","date":"2019-02-23T13:37:57.986Z","path":"wiki/工具组件/前端方案/前端框架记录/","text":"https://antd-admin.zuiidea.com/zh/dashboard","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"前端方案","slug":"工具组件/前端方案","permalink":"http://wiki.quartz.ren/categories/工具组件/前端方案/"}]},{"title":"","date":"2019-02-23T13:37:57.984Z","path":"wiki/MicroService/README/","text":"Spring Boot 2.0深度实践之核心技术","tags":[],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"}]},{"title":"开发工具","date":"2019-01-08T03:55:57.000Z","path":"wiki/工具组件/IDE/开发工具/","text":"好用的开发工具dbeaver https://nosqlbooster.com/downloads","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"IDE","slug":"工具组件/IDE","permalink":"http://wiki.quartz.ren/categories/工具组件/IDE/"}]},{"title":"ES介绍","date":"2018-12-15T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES介绍/","text":"Es介绍","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"Logstash安装","date":"2018-12-15T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/Logstash安装/","text":"logstash是做数据采集的，类似于flume。 官网logstash介绍 下载地址 解压后执行一下命令,查看效果: 1bin/logstash -e &apos;input &#123; stdin &#123; &#125; &#125; output &#123; stdout &#123;&#125; &#125;&apos; 安装logstash-input-jdbc参考 logstash-input-jdbc插件是logstash 的一个个插件。 使用ruby语言开发. 安装gem, 替换淘宝镜像 123456安装gemsudo apt install gemsudo apt install rubygem -vgem sources --add https://gems.ruby-china.com/ --remove https://rubygems.org/gem sources -l RubyGems镜像站 查看logstash可用插件 1bin/logstash-plugin list --verbose 以上找到对应的版本. 查看相应的文档https://www.elastic.co/guide/en/logstash-versioned-plugins/current/v4.3.9-plugins-inputs-jdbc.html 安装命令 1bin/logstash-plugin install logstash-input-jdbc 使用实现mysql数据同步到Elasticsearch 需要一个mysql驱动包，sql文件,以及conf配置文件 sql文件 bank_sync.sql 1234567SELECT t.id, t.`code`, t.`name`, t.per_day_limitFROM tb_bank_type t mysql.conf文件 12345678910111213141516171819202122232425262728293031323334353637input &#123; jdbc &#123; # mysql jdbc connection string to our backup databse jdbc_connection_string =&gt; &quot;jdbc:mysql://127.0.0.1:3306/carinsurance&quot; # the user we wish to excute our statement as jdbc_user =&gt; &quot;carinsurance&quot; jdbc_password =&gt; &quot;123456&quot; # the path to our downloaded jdbc driver jdbc_driver_library =&gt; &quot;/opt/elasticsearch/logstash-6.2.2/sql/mysql-connector-java-5.1.40.jar&quot; # the name of the driver class for mysql jdbc_driver_class =&gt; &quot;com.mysql.jdbc.Driver&quot; jdbc_paging_enabled =&gt; &quot;true&quot; jdbc_page_size =&gt; &quot;50000&quot; statement_filepath =&gt; &quot;/opt/elasticsearch/logstash-6.2.2/sql/bank_sync.sql&quot; schedule =&gt; &quot;*/1 * * * *&quot; type =&gt; &quot;jdbc&quot; &#125;&#125;filter &#123; json &#123; source =&gt; &quot;message&quot; remove_field =&gt; [&quot;message&quot;] &#125;&#125;output &#123; elasticsearch &#123; hosts =&gt; &quot;127.0.0.1:9200&quot; index =&gt; &quot;bank&quot; document_id =&gt; &quot;%&#123;id&#125;&quot; &#125; stdout &#123; codec =&gt; json_lines &#125;&#125; 上面配置文件中的sql文件和mysql驱动注意路径正确 启动logstash 123bin/logstash -f mysql.conf 后台启动: nohup ./logstash -f mysql.conf &gt; /dev/null 2&gt;&amp;1 &amp;nohup bin/logstash -f sync-data/mysql.conf &amp; 注意: es需要外网访问,同kibana,需要配置如下: 1network.host: 0.0.0.0 配置以上后出现问题: 系统最大文件描述符限制,最大虚拟内存限制 解决: 12345678vi /etc/security/limits.conf 将65535 改为65536root用户执行以下:vi /etc/sysctl.conf 添加一下配置vm.max_map_count=655360使其生效sysctl -p 应用【技术实验】mysql准实时同步数据到Elasticsearch 全文搜索引擎 Elasticsearch （三）logstash-input-jdbc同步数据 到elasticsearch","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"Redis常用操作","date":"2018-12-14T17:55:57.000Z","path":"wiki/数据处理/DataStore/Redis/Redis常用操作/","text":"Redis开发运维实践指南","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Redis","slug":"数据处理/DataStore/Redis","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Redis/"}]},{"title":"Docker 仓库搭建","date":"2018-12-14T13:55:57.000Z","path":"wiki/工具组件/Docker/002.Docker镜像仓库搭建/","text":"123456789101112mkdir -p /opt/data/registry //创建目录docker run -d \\ -p 5000:5000 \\ --restart=always \\ --name docker-registry \\ -v /data/docker-registry:/var/lib/registry \\ registry:2-d : 让容器可以后台运行-p ：指定映射端口（前者是宿主机的端口号，后者是容器的端口号）-v ：数据挂载（前者是宿主机的目录，后者是容器的目录）--name : 为运行的容器命名","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"Docker NetWork","date":"2018-12-12T03:55:57.000Z","path":"wiki/工具组件/Docker/001.Docker网络/","text":"docker容器跨主机互联小实验 docker 多种跨主机访问选择哪一种 理解Docker跨多主机容器网络 Docker rabbit1docker run -d --hostname my-rabbit --name rabbit -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=password -p 15672:15672 -p 5672:5672 -p 25672:25672 -p 61613:61613 -p 1883:1883 rabbitmq:management Docker redis1docker run -d --name redis-server -p 6379:6379 redis --requirepass &quot;password&quot;","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Docker","slug":"工具组件/Docker","permalink":"http://wiki.quartz.ren/categories/工具组件/Docker/"}]},{"title":"ELK","date":"2018-12-09T03:55:57.000Z","path":"wiki/数据处理/数据收集/ELK/","text":"Docker快速搭建elk服务镜像","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"数据收集","slug":"数据处理/数据收集","permalink":"http://wiki.quartz.ren/categories/数据处理/数据收集/"}]},{"title":"Python常用函数","date":"2018-12-09T03:55:57.000Z","path":"wiki/程序语言/Python/基础操作/","text":"shape读取矩阵的长度. shape函数是numpy.core.fromnumeric中的函数，它的功能是读取矩阵的长度，比如shape[0]就是读取矩阵第一维度的长度。shape的输入参数可以是一个整数（表示维度），也可以是一个矩阵。以下例子可能会好理解一些： Python numpy函数：shape用法 mat创建矩阵 python中的mat的操作","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"其他算法","date":"2018-12-07T14:55:57.000Z","path":"wiki/机器学习/其他算法/","text":"Java中的字符串相似度","tags":[],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"Tcp,Ip及其应用","date":"2018-12-04T15:11:57.000Z","path":"wiki/软件工程/计算机网络/TCP-IP及应用问题/","text":"SocketRead问题针对线程处于 at java.net.SocketInputStream.socketRead0(Native Method) , java.lang.Thread.State: RUNNABLE. 123456789&quot;http-nio-10251-exec-1213&quot; #86023 daemon prio=5 os_prio=0 tid=0x00007f838c0f8000 nid=0x1f9 runnable [0x00007f83693bd000] java.lang.Thread.State: RUNNABLE at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:171) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.security.ssl.InputRecord.readFully(InputRecord.java:465)... 关于TCP交互流程与客户端服务端状态 TimeWait,CloseWait 有没有问题","tags":[],"categories":[{"name":"软件工程","slug":"软件工程","permalink":"http://wiki.quartz.ren/categories/软件工程/"},{"name":"计算机网络","slug":"软件工程/计算机网络","permalink":"http://wiki.quartz.ren/categories/软件工程/计算机网络/"}]},{"title":"Kafka问题总结","date":"2018-12-04T15:11:57.000Z","path":"wiki/数据处理/DataStore/Kafka/关于最近kafka的几个问题/","text":"consumer提交offset失败使用spring-kafka, 配置auto.commit =true 会使用kafka.client的 自动提交机制(5秒钟提交一次-具体细节TODO). 然而一定时间取出的消息没有处理完,长时间没能提交成功??? 现象: offset没有提交成功, consumer的协调者处于dead状态, 恢复后但还能正常消费,但offset一直未能提交. Socket一个线程导致消费挂起现象: Socket一直处于read状态,导致往队列push消息失败,取出来的消息也没有处理, 没有提交offset. 可能1. socket read timeout 设置后, 其他线程就处于 Timewait 状态,但是 实际设置connect 超时时间在 socket 超时时间之后, 所以没有超时时间相当于.","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"Logistic回归","date":"2018-12-02T14:55:57.000Z","path":"wiki/机器学习/Logistic回归/","text":"概述回归: 假设有一组数据点,用一条直线对这些点进行拟合,这个拟合过程就是回归.回归一词源于最佳拟合,表示要找到最佳拟合参数. 训练分类器的做法就是寻找最佳拟合参数. 使用的一些优化算法: 梯度上升法,最小二乘法. 最优化算法1. 最小二乘法向量运算进行参数求解过程 损失函数,给定数据X,Y, 根据aX 的出的Y1 与Y之间的偏差,称为损失.怎么将这个损失降到最低. 首先定义这损失,线性空间的距离,通过欧几里得距离定义这个损失. 损失最小化: 对损失函数求导,得到参数方程. 进行参数计算. 2. 梯度下降为什么梯度下降是必须的? 最小二乘法参数计算的问题: 矩阵是否满秩 运算性能 梯度下降不仅限于线性回归. 经过多次的重复, 比直接运算(参数计算)的优点.","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"KNN","date":"2018-12-02T03:55:57.000Z","path":"wiki/机器学习/KNN/","text":"概述简单的说,K-近邻算法采用测量不同特征值之间的距离方法进行分类. 工作原理: 存在一个样本数据集合, 也称作训练样本集, 并且样本集中的每个数据都存在标签(即我们知道样本集中每一数据与所属分类的对应关系),输入没有标签的新数据后.将新数据的每个特征与样本集中数据对应的特征进行比较,然后算法提取样本集中特征最相似数据(最近邻)的分类标签. 特性: 优点: 精度高,对异常值不敏感,无数据输入假设 缺点,计算复杂度高,空间复杂度高 使用数据范围: 数值型和标称型. 举例使用KNN 分类爱情片和动作片.基于电影中出现的亲吻,打斗出现的次数,使用k-近邻构造程序. 训练样本集: A 打斗镜头: 3 接吻镜头: 104 电影类型 : 爱情片B 打斗镜头: 2 接吻镜头: 100 电影类型 : 爱情片C 打斗镜头: 1 接吻镜头: 81 电影类型 : 爱情片D 打斗镜头: 101 接吻镜头: 10 电影类型 : 动作片E 打斗镜头: 99 接吻镜头: 5 电影类型 : 动作片F 打斗镜头: 98 接吻镜头: 2 电影类型 : 动作片G 打斗镜头: 18 接吻镜头: 90 电影类型 : ? 已知类型电影 (A,B,C,D,E,F) 与 未知类型电影(G) 的距离如下. 距离怎么定义的?? (二维空间的绝对距离?) A 与 G 的距离 : 20.5B 与 G 的距离 : 18.7C 与 G 的距离 : 19.2D 与 G 的距离 : 115.3E 与 G 的距离 : 117.4F 与 G 的距离 : 118.9 现在得到了样本集中所有电影与未知电影的距离,按照距离递增排序, 可以找到k个距离最近的电影.假设k=3, 则最近的三个电影依次为: F,E,D. 而 这三个都是爱情片, 因此我们判断未知电影为爱情片. 算法实现 收集数据 准备数据: 距离计算所需要的数值,最好是结构化的数据格式 分析数据 训练算法: k近邻不适用 测试算法: 计算错误率 使用算法 这里只介绍最后的算法使用步骤:实施KNN分类算法. 主要函数功能为: 使用k-近邻算法将每组数据划分到某个类别中.步骤如下 123456对未知类别属性的数据集中的每个点依次执行以下操作:1. 计算已知类别数据集中的点与当前点之间的距离2. 按照距离递增次序排序3. 选取与当前点距离最小的k个点4. 确定前k个点所在类别的出现频率 5. 返回前k个点出现频率最高的类别作为当前点的预测分类. python函数如下: 1234567891011121314def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] // 距离计算 diffMat = tile(inX, (dataSetSize, 1)) - dataSet sqDiffMat = diffMat ** 2 sqDistances = sqDiffMat.sum(axis=1) disstances = sqDistances.argsort() classCount=&#123;&#125; // 选择距离最小的k个点 for i in rang(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 其它代码见:https://github.com/quantumcs/Machine-Learning-In-Action 算法测试有关数据收集 文本数据的转换及处理etc. 其它的一些应用 约会网站的配对 手写识别系统 问题1. 一个简单的模型,存在的问题现有一个模型, 针对每个新的句子,使用相似度算出与已知样本集中每个句子的相似度. 而已知样本集只有一个类别(也就是说都是负样本). 取最大的相似度值,和阈值比较,大于阈值的定义同一个类别,小于阈值的定为不同类别. 存在的问题: 1. 没有考虑正样本,这个策略效果肯定存在提升. 2. 另外的提升思路: 对句子做预处理,去除变化较大的实体,降低对相似性的影响.","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"AdaBoost算法","date":"2018-12-02T03:55:57.000Z","path":"wiki/机器学习/利用AdaBoost元算法提高分类性能/","text":"组合相似的分类器来提高分类性能 应用AdaBoost算法 处理非均衡分类问题 概述元算法: 对其他算法进行组合的一种方式. 代表-AdaBoost 不同分类器的集成方法: boosting方法(代表-AdaBoost) 非均衡分类问题 基于数据集多重抽样的分类器学习了常见的分类算法:KNN,决策树,朴素贝叶斯,logistic回归.他们各有优缺点,可以将不同的分类器进行组合,而这种组合结果就被称为集成方法或者元算法. 1. bagging基于数据随机重抽样的分类器构建方法 是从原始数据集选择S次后得到S个数据集的一种技术,新数据集和原数据集的大小相等. 一种先进的bagging方法 - 随机森林 2. boosting类似bagging技术. 不管是boosting还是bagging,所使用的多个分类器的类型都是一致的,但是在前者当中,不同分类器是通过串行训练而获得的,每个新分类器都根据已训练出的分类器的性能进行训练. boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器. 由于boosting分类的结果是基于所有分类器的加权求和结果的,因此boosting与bagging不太一样. bagging中的分类器权重是相等的,而boosting中的分类器权重并不相等,每个权重代表的是其对应分类器在上一轮迭代中的成功度. boosting方法拥有多个版本,本文只关注一个最流行的版本AdaBoost. AdaBoost一般流程 1234561. 收集数据2. 准备数据: 依赖于所使用的弱分类器类型,比如单层决策树,这种分类器可以处理任何数据类型.(作为弱分类器,简单分类器的效果更好)3. 分析数据4. 训练算法: AdaBoost的大部分时间都用在训练上,分类器将多次在同一数据集上训练弱分类器5. 测试算法: 计算分类的错误率6. 使用算法:同SVM一样,AdaBoost预测两个类别中的一个.如果想把它应用到多个类别的场合,那么就要像多累SVM中的做法一样对AdaBoost进行修改 关于分类性能度量指标:正确率\\召回率及ROC曲线. 128p. 基于代价函数的分类器决策控制. p131 处理非均衡问题的数据抽样方法 p132 代码:https://github.com/quantumcs/Machine-Learning-In-Action/tree/master/Ch07","tags":[{"name":"MchineLearning","slug":"MchineLearning","permalink":"http://wiki.quartz.ren/tags/MchineLearning/"}],"categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://wiki.quartz.ren/categories/机器学习/"}]},{"title":"数据处理相关","date":"2018-12-02T03:55:57.000Z","path":"wiki/数据处理/技术文章/Start/","text":"https://www.iteblog.com/archives/1947.html","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"技术文章","slug":"数据处理/技术文章","permalink":"http://wiki.quartz.ren/categories/数据处理/技术文章/"}]},{"title":"sbt了解","date":"2018-12-02T03:55:57.000Z","path":"wiki/工具组件/Builder/sbt/","text":"下载安装:https://www.scala-sbt.org/download.html sbt是类似ANT、MAVEN的构建工具，全称为Simple build tool，是Scala事实上的标准构建工具。 主要特性: 原生支持编译Scala代码和与诸多Scala测试框架进行交互； 使用Scala编写的DSL（领域特定语言）构建描述 使用Ivy作为库管理工具 持续编译、测试和部署 整合scala解释器快速迭代和调试 支持Java与Scala混合的项目 加速 SBT 下载依赖库的速度 sbt介绍与构建Scala项目","tags":[],"categories":[{"name":"工具组件","slug":"工具组件","permalink":"http://wiki.quartz.ren/categories/工具组件/"},{"name":"Builder","slug":"工具组件/Builder","permalink":"http://wiki.quartz.ren/categories/工具组件/Builder/"}]},{"title":"Python学习","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Python/Python语法学习笔记/","text":"语言比较 有非常完善的基础代码库 定位:“优雅”、“明确”、“简单” Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。如果一个资深程序员向你炫耀他写的晦涩难懂、动不动就几万行的代码，你可以尽情地嘲笑他。 可以做什么? 网站、后台服务等都可以 数据类型在内存中存储的数据可以有多种类型。 例如，一个人的年龄可以用数字来存储，他的名字可以用字符来存储。 Python 定义了一些标准类型，用于存储各种类型的数据。 Python有五个标准的数据类型： Numbers（数字） String（字符串） List（列表） Tuple（元组） Dictionary（字典） 基本类型及Python数据类型转换 1. List1list = [ 'runoob', 786 , 2.23, 'john', 70.2 ] 列表可以完成大多数集合类的数据结构实现。它支持字符，数字，字符串甚至可以包含列表（即嵌套）。列表用 [ ] 标识，是 python 最通用的复合数据类型。列表中值的切割也可以用到变量 [头下标:尾下标] ，就可以截取相应的列表，从左到右索引默认 0 开始，从右到左索引默认 -1 开始，下标可以为空表示取到头或尾。 2. Tuple1tuple = ( 'runoob', 786 , 2.23, 'john', 70.2 ) 元组是另一个数据类型，类似于List（列表）。 元组用”()”标识。内部元素用逗号隔开。但是元组不能二次赋值，相当于只读列表。 3. Dictionary1234dict = &#123;&#125;dict['one'] = \"This is one\"dict[2] = \"This is two\"tinydict = &#123;'name': 'john','code':6734, 'dept': 'sales'&#125; 4. 数据类型转换Python数据类型转换 其他Python 内置函数 Python GUI编程(Tkinter) Python JSON","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"Scala项目构建","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Scala/使用maven构建scala项目/","text":"SCALA WITH MAVEN 123mvn archetype:generate输入 groupId等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.11.8&lt;/version&gt; &lt;/dependency&gt;&lt;build&gt; &lt;plugins&gt; &lt;!-- This plugin compiles Scala files --&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile-first&lt;/id&gt; &lt;phase&gt;process-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;add-source&lt;/goal&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;phase&gt;process-test-resources&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- This plugin compiles Java files --&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!-- This plugin adds all dependencies to JAR file during 'package' command. Pay EXTRA attention to the 'mainClass' tag. You have to set name of class with entry point to program ('main' method) --&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;ScalaRunner&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; scala和maven整合实践","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Scala","slug":"程序语言/Scala","permalink":"http://wiki.quartz.ren/categories/程序语言/Scala/"}]},{"title":"Scala学习","date":"2018-12-02T03:55:57.000Z","path":"wiki/程序语言/Scala/Scala入门/","text":"有了java,为什么要用scala.想学scala. 先了解下它有什么比java更优秀的地方. Scala是2001年诞生的一门多范式语言 .设计初衷是要集成面向对象编程和函数式编程的各种特性 Scala 特性 面向对象特性 函数式编程 静态类型 扩展性 并发","tags":[],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Scala","slug":"程序语言/Scala","permalink":"http://wiki.quartz.ren/categories/程序语言/Scala/"}]},{"title":"Kafak监控","date":"2018-12-02T03:55:57.000Z","path":"wiki/数据处理/DataStore/Kafka/Kakfa监控/","text":"推荐的监控工具kafka-offset-monitor Python实现监控123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196topics = ['heimdallr-dev']# 要监控的groupidmonitor_group_ids = ['heimdallr']# broker-listservers = 'localhost:9092'# 监控数据上报间隔 秒time_interval = 10# 历史全量数据上报间隔history_time_interval = 5 * 60# -*- coding:utf-8 -*-import timeimport sysfrom kafka.client import KafkaClientfrom kafka.protocol.commit import OffsetFetchRequest_v1, OffsetFetchResponse_v1, OffsetFetchRequest_v0, \\ OffsetFetchResponse_v0from kafka.protocol.offset import OffsetRequest_v0, OffsetResponse_v0#from monitor_constants import *duration = 0client = Noneconn = Nonepartition_cache = &#123;&#125;brokers_cache = []kafka_type = ['heimdallr']zk_type = []def get_brokers(): if not brokers_cache: brokers = client.cluster.brokers() if brokers: brokers_cache.extend([x.nodeId for x in brokers]) return brokers_cachedef get_partitions(topic): if not partition_cache or topic not in partition_cache: partitions = client.cluster.available_partitions_for_topic(topic) if partitions: partition_cache[topic] = [x for x in partitions] else: return [] return partition_cache[topic]def get_logsize(): \"\"\" 获取topic 下每个partition的logsize(各个broker的累加) :return: \"\"\" tp = &#123;&#125; # topic : partition_dict brokers = get_brokers() for topic in topics: partitions = get_partitions(topic) pl = &#123;&#125; # partition : logsize for broker in brokers: # 这里取笛卡尔积可能有问题,但是不影响parse中解析了 for partition in partitions: client.send(broker, OffsetRequest_v0(replica_id=-1, topics=[(topic, [(partition, -1, 1)])])) responses = client.poll() pdict = parse_logsize(topic, partition, responses) append(pl, pdict) tp[topic] = pl return tpdef append(rdict, pdict): if rdict: # 已经有记录,累加 for k, v in pdict.items(): if k in rdict: rdict[k] = rdict[k] + v else: rdict[k] = v else: rdict.update(pdict)def parse_logsize(t, p, responses): \"\"\" 单个broker中单个partition的logsize :param responses: :param p: :param t: :return: \"\"\" for response in responses: if not isinstance(response, OffsetResponse_v0): return &#123;&#125; tps = response.topics topic = tps[0][0] partition_list = tps[0][1] partition = partition_list[0][0] # 异步poll来的数据可能不准 if topic == t and partition == p and partition_list[0][1] == 0: logsize_list = partition_list[0][2] logsize = logsize_list[0] return &#123;partition: logsize&#125; return &#123;&#125;def parse_offsets(t, responses): dr = &#123;&#125; for response in responses: if not isinstance(response, (OffsetFetchResponse_v1, OffsetFetchResponse_v0)): return &#123;&#125; tps = response.topics topic = tps[0][0] partition_list = tps[0][1] if topic == t: for partition_tunple in partition_list: if partition_tunple[3] == 0: offset = partition_tunple[1] dr[partition_tunple[0]] = offset return drdef get_offsets(): # &#123;gid: dict&#125; gd = &#123;&#125; for gid in monitor_group_ids: td = &#123;&#125; # &#123;topic:dict&#125; for topic in topics: pd = &#123;&#125; # &#123;partition:dict&#125; for broker in get_brokers(): partitions = get_partitions(topic) if not partitions: return &#123;&#125; else: responses = optionnal_send(broker, gid, topic, partitions) dr = parse_offsets(topic, responses) append(pd, dr) td[topic] = pd gd[gid] = td return gddef optionnal_send(broker, gid, topic, partitions): if gid in kafka_type: return kafka_send(broker, gid, topic, partitions) elif gid in zk_type: return zk_send(broker, gid, topic, partitions) else: responses = zk_send(broker, gid, topic, partitions) dct = parse_offsets(topic, responses) if is_suitable(dct): zk_type.append(gid) return responses responses = kafka_send(broker, gid, topic, partitions) dct = parse_offsets(topic, responses) if is_suitable(dct): kafka_type.append(gid) return responsesdef is_suitable(dct): for x in dct.values(): if x != -1: return Truedef kafka_send(broker, gid, topic, partitions): client.send(broker, OffsetFetchRequest_v1(consumer_group=gid, topics=[(topic, partitions)])) return client.poll()def zk_send(broker, gid, topic, partitions): client.send(broker, OffsetFetchRequest_v0(consumer_group=gid, topics=[(topic, partitions)])) return client.poll()def do_task(): offset_dict = get_offsets() #print (offset_dict) logsize_dict = get_logsize() #print (logsize_dict) print ('----------kafka monitor, info:-------------') for gk, gv in offset_dict.items(): for tk, tv in gv.items(): for pk, pv in tv.items(): if logsize_dict and tk in logsize_dict: dr = logsize_dict[tk] # partition:logsize if dr and pk in dr: param = (gk, tk, pk, pv, dr[pk], time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))) print(param)if __name__ == \"__main__\": client = KafkaClient(bootstrap_servers=servers, request_timeout_ms=3000) while True: do_task() time.sleep(time_interval) duration += time_interval 参考:https://my.oschina.net/ktlb/blog/863308","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"关于kafka的offset存储","date":"2018-11-24T17:36:57.000Z","path":"wiki/数据处理/DataStore/Kafka/关于kafka的offset存储/","text":"消费者通过offset控制消费的进度,这里有几个概念先解释一下. Offset: 每个ConsumerGroup中针对一个topic的每个Partition的消费进度.通过这个来控制消费进度. LogSize: Kafka的数据位置,随着新的数据到来而增加. Lag: LogSize - Offset . 指落后的大小. 因此正常Consumer的不堆积是Lag的值处于比较小的范围,比如 0~1000. 然而,存在的一些问题: 那随着数据量的增加,offset和logSize的值一直增加,到超过int的范围吗,还是有清零的规则.(应该是有相应的机制,这个不重要了) 有关offset的一些注意点如下 存储位置从kafka-0.9版本及以后,消费者组和offset信息就不存在zk中了,而是存到broker服务器上.存放在一个叫__consumer_offsets的topic中. 关于offset的消费者参数auto.offset.reset 123456earliest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常 也就是说,这个参数的指定只有在新的consumer group添加的时候,或者其他原因导致分区上的offset没有了的情况,才更有意义. 那随之又有的问题: 如果为了能消费新的数据,而对于老的customer-group,不想消费堆积的数据. 或者说想废弃掉这个group了,那不用之后会有什么影响 另外,对于无止尽的customer-group创建,对kafka集群有什么影响吗,当然不仅仅是新group替代旧的group.而是还有在用group的增多,会对集群有什么影响? 下面详细总结下 1. 废弃group的增多个人理解: group增多,增加了对group的管理成本,那对于不用的group,存放在broker中,不会对其它造成影响.目前只是猜测,具体再详细研究. 2. 在用group的增多对于老版本(zk管理customer信息和offset), 会增加customer与zk的交互成本. 新版本(大于0.9), customer信息和offset由broker管理,只是增加了customer与broker的交互, 然而这一部分交互信息对于整个数据流来说微乎其微,所以影响应该不大. 需要在研究下offset更新的流程(customer与broker) 再聊聊kafka的group coordinator Coordinator一般指的是运行在broker上的group Coordinator，用于管理Consumer Group中各个成员，每个KafkaServer都有一个GroupCoordinator实例，管理多个消费者组，主要用于offset位移管理和Consumer Rebalance。 在 Server 端增加了 GroupCoordinator 这个角色 将 topic 的 offset 信息由之前存储在 zookeeper(/consumers/&lt;group.id&gt;/offsets//,zk写操作性能不高) 上改为存储到一个特殊的 topic 中（__consumer_offsets） 1. rebalance时机 有新的consumer加入 旧的consumer挂了 coordinator挂了，集群选举出新的coordinator topic的partition新加 consumer调用unsubscrible()，取消topic的订阅 关于offset的提交,管理 2. __consumer_offsetsConsumer通过发送OffsetCommitRequest请求到指定broker（偏移量管理者）提交偏移量。 这个请求中包含一系列分区以及在这些分区中的消费位置（偏移量） 偏移量管理者会追加键值（key－value）形式的消息到一个指定的topic（__consumer_offsets）。key是由consumerGroup-topic-partition组成的，而value是偏移量。 感觉其实用HashMap应该更好一些,因为通过key来获取或管理offset(偏移量-value) 因为这种存储方式(队列), find的时间复杂度为O(n), 需要遍历整个__consumer_offsets,扫描全部偏移量topic日志. 因此集群的内存中也是维护了一份最近的记录,为了能在指定key的情况下能够快速的给出OffsetFetchRequests而不用扫描全部偏移量topic日志. 如果偏移量管理者因某种原因失败，新的broker将会成为偏移量管理者并且通过扫描偏移量topic来重新生成偏移量缓存。 ps: 内存中应该是Map结构,那内存中的记录与偏移量topic(__consumer_offsets)的数据怎么保证一致性的呢?? 3. Consumer与Consumer Groupconsumer group是kafka提供的可扩展且具有容错性的消费者机制。组内可以有多个消费者或消费者实例(consumer instance)，它们共享一个公共的ID，即group ID。组内的所有消费者协调在一起来消费订阅主题(subscribed topics)的所有分区(partition)。 consumer instance可以是一个进程，也可以是一个线程. 有关offset的几个概念Kafka 之 Group 状态变化分析及 Rebalance 过程","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"Kafak环境搭建","date":"2018-11-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Kafka/Kafka环境搭建/","text":"下载kafkahttp://kafka.apache.org/quickstart 启动zookeeperbin/zookeeper-server-start.sh config/zookeeper.properties &gt;&gt; zookeeper.out 2&gt;&amp;1 &amp; 启动kafka单机bin/kafka-server-start.sh config/server.properties &gt;&gt;kafka.out 2&gt;&amp;1 &amp; 集群启动bin/kafka-server-start.sh config/server.properties &gt;&gt;kafka-0.out 2&gt;&amp;1 &amp;bin/kafka-server-start.sh config/server-1.properties &gt;&gt;kafka-1.out 2&gt;&amp;1 &amp;bin/kafka-server-start.sh config/server-2.properties &gt;&gt;kafka-2.out 2&gt;&amp;1 &amp; 监控Kafka三款监控工具比较 Kafka监控工具KafkaOffsetMonitor配置及使用 下载KakfaOffsetMonitor 12345java -Xms512M -Xmx512M -Xss1024K -XX:PermSize=256m -XX:MaxPermSize=512m -cp KafkaOffsetMonitor-assembly-0.2.0.jar com.quantifind.kafka.offsetapp.OffsetGetterWeb \\--port 8088 \\--zk 10.0.0.50:12181,10.0.0.60:12181,10.0.0.70:12181 \\--refresh 5.minutes \\--retain 1.day &gt;/dev/null 2&gt;&amp;1; TODO 监控原理 PS: kafka 日志默认保存7天. topic创建1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 5 --topic my-replicated-topic 命令行消费者1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic nginx_log --from-beginning ubuntu命令行设置系统代理 增加partition1bin/kafka-topics.sh --alter --zookeeper 127.0.0.1:2181 --partitions 10 --topic nginx_log 查看某个topic的 logSize指的是topic各个分区的logSize 1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --topic nginx_log --time -1 time 为-2 表示查看offset的最小值, -1 表示最大值1bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 -topic nginx_log --time -2 查看消费者组内的offset位置(消费情况)关于kafka更改消费者对应分组下的offset值 12# To view offsets, as mentioned earlier, we &quot;describe&quot; the consumer group like this:bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --group consumer02 --describe Managing Consumer GroupsManaging Consumer Groups 123456# bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --list# provides the list of all active members in the consumer group.bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --describe --group consumer02 --members# bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --describe --group consumer02 --state 更改offset1234# 先查看一下customer的offset状态bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --group consumer02 --describe# reset offsets of a consumer group to the latest offset: (earliest)bin/kafka-consumer-groups.sh --bootstrap-server api.quartz.ren:9092,api.quartz.ren:9093,api.quartz.ren:9094 --reset-offsets --group consumer02 --topic nginx_log --to-latest 以上reset 只能在 consumer inactive状态时,才可以. 问题: 这个操作的目的和结果是什么??? Kafka auto.offset.reset值详解 123456earliest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费 latest 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据 none topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Kafka","slug":"数据处理/DataStore/Kafka","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Kafka/"}]},{"title":"pip 模块安装","date":"2018-11-17T09:10:49.000Z","path":"wiki/程序语言/Python/模块安装/","text":"pip安装1apt install python3-pip 安装完pip就可以使用它来安装需要的模块。也可以指定安装目录1pip install xgboost --target=/home/work/.local/lib/python2.7/site-packages/ -i https://mirrors.aliyun.com/pypi/simple/","tags":[{"name":"Python","slug":"Python","permalink":"http://wiki.quartz.ren/tags/Python/"}],"categories":[{"name":"程序语言","slug":"程序语言","permalink":"http://wiki.quartz.ren/categories/程序语言/"},{"name":"Python","slug":"程序语言/Python","permalink":"http://wiki.quartz.ren/categories/程序语言/Python/"}]},{"title":"Docker相关","date":"2018-11-16T17:13:34.000Z","path":"wiki/MicroService/Docker/index/","text":"Deepin下安装Docker 阿里云.来自云端的容器镜像服务 Docker镜像加速：阿里云 Docker常用操作命令 查看日志123456docker logs -f -t --since=&quot;2017-05-31&quot; --tail=10 edu_web_1--since : 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。-f : 查看实时日志-t : 查看日志产生的日期-tail=10 : 查看最后的10条日志。edu_web_1 : 容器名称 Docker 容器自定义 hosts 网络访问Docker自定义hosts网络访问 需求：使用自己的域名服务。 在很多应用环境中都会有其他服务器的访问需求。直接使用ip不便于修改。因此搭建自己的域名服务，之后配置docker环境使用此域名服务即可。也可同时使用其它域名服务。 12345(1)resolv-file=/etc/resolv.conf(2)strict-order(3)listen-address=192.168.0.127,127.0.0.1(4)addn-hosts=/etc/hosts //这歌地址指向的是你mac的hosts地址，你只需在里边做相应的 host(5)cache-size=2048 Docker镜像管理 搭建Docker私有仓库 更改Docker环境的配置指向私有仓库 向私有仓库提交镜像 从另外的docker环境获取上述镜像 Docker的镜像归纳为两种 需要知道docker容器的地址的类似zk这种组件的 不需要知道docker容器地址类似应用程序通过注册服务到zk，然后自动发现服务。 docker查看运行容器ip1docker inspect 容器ID | grep IPAddress","tags":[{"name":"Docker","slug":"Docker","permalink":"http://wiki.quartz.ren/tags/Docker/"}],"categories":[{"name":"MicroService","slug":"MicroService","permalink":"http://wiki.quartz.ren/categories/MicroService/"},{"name":"Docker","slug":"MicroService/Docker","permalink":"http://wiki.quartz.ren/categories/MicroService/Docker/"}]},{"title":"ES基础","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES基础/","text":"Docker安装esInstall Elasticsearch with Docker es 建议先手动安装. head 插件可以使用doker创建 12docker pull elasticsearchdocker run -d -p 9200:9200 --name=es-server elasticsearch 使用另外一个镜像做head插件 1docker run --name elasticsearch-head -d -p 9100:9100 mobz/elasticsearch-head:5 使用docker可视化界面管理容器 1docker run --name docker-ui -d -p 9000:9000 --privileged -v /var/run/docker.sock:/var/run/docker.sock uifd/ui-for-docker ES安装下载 elasticsearch-6.3.2.tar.gz. 1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.3.2.zip 解压后:修改配置文件,添加 12http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; 上述是由于head插件和es 之间存在跨域问题(两个进程),需要给权限 https://www.jianshu.com/p/f80fb1dd842b 12# 启动bin/elasticsearch -d 安装插件analysis-ik1./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v6.3.2/elasticsearch-analysis-ik-6.3.2.zip https://github.com/medcl/elasticsearch-analysis-ik 安装sql插件1./bin/elasticsearch-plugin install https://github.com/NLPchina/elasticsearch-sql/releases/download/6.3.2.0/elasticsearch-sql-6.3.2.0.zip https://github.com/NLPchina/elasticsearch-sql 其他插件https://www.elastic.co/guide/en/elasticsearch/plugins/6.3/analysis.html 安装kibana1wget https://artifacts.elastic.co/downloads/kibana/kibana-6.3.2-linux-x86_64.tar.gz http://www.quartz.ren:5601/app/kibana#/home?_g=())","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"ES常用API","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Elasticsearch/ES常用api/","text":"常用API 查看所有索引1http://yun.quartz.ren:9200/_cat/indices?v 索引和搜索文档索引里面还有类型的概念，在索引文档之前要先设置类型type 查询所有文档(索引的所有文档) 1234GET /lagou_job/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 查看索引结构其他语言的APIpython API","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Elasticsearch","slug":"数据处理/DataStore/Elasticsearch","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Elasticsearch/"}]},{"title":"使用python添加数据到mongo","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Mongo/使用python添加数据到Mongo/","text":"123456789101112131415161718192021222324252627282930313233343536import pymongoimport jsonclient = pymongo.MongoClient(\"localhost\", 27017)db = client.test# print (db.name)# print (db.my_collection)# db.my_collection.insert_one(&#123;\"x\": 10&#125;).inserted_id# create collection . 可以不用创建,插入数据会自动创建## db.createCollection(\"UserBehavior\", &#123; capped : true, autoIndexId : true, size : 6142800, max : 100000 &#125; )fo = open(\"/home/zhangquanquan/workspace/stream-project/my-flink-project/src/main/resources/UserBehavior.csv\", \"r\")print (\"文件名为: \", fo.name)fileList = fo.readlines()fo.close()for line in fileList: line = line.strip() lineItem = line.split(\",\") ub = &#123;&#125; ub['userId'] = lineItem[0] ub['itemId'] = lineItem[1] ub['categoryId'] = lineItem[2] ub['behavior'] = lineItem[3] ub['timestamp'] = lineItem[4] json_ub = json.dumps(ub) db.user_behavior.insert(ub) # 批量插入,可以使用 db.collection.insertMany([&#123;\"b\": 3&#125;, &#123;'c': 4&#125;]) # print (json_ub)print (\"finished.\")# for item in db.my_collection.find().sort(\"x\", pymongo.ASCENDING):# print(item[\"x\"])","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Mongo","slug":"数据处理/DataStore/Mongo","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Mongo/"}]},{"title":"Mongo基础","date":"2018-01-21T17:55:57.000Z","path":"wiki/数据处理/DataStore/Mongo/Mongo基础/","text":"Docker安装mongo12docker pull mongodocker run --name mongo-server -p 27017:27017 -v /data/mongo/db:/data/db -d mongo 使用mongo镜像执行mongo命令了连接容器 1docker run -it mongo mongo --host 172.17.0.2 相关命令1234命令行登陆数据库（类似 mysql -uroot -p -h）mongo --port 27017use admindb.auth(&quot;adminUser&quot;, &quot;adminPass&quot;) 1234567891011&gt; 查看数据库show databases; // 或者 show dbs;&gt; 创建数据库use new_db; //只有插入数据,db才会真正创建.&gt; 创建集合后要再插入一个文档(记录)db.test.insert(&#123;&quot;name&quot;:&quot;菜鸟&quot;&#125;)&gt; 删除数据库db; // 查看当前数据库db.dropDatabase();//删除当前数据库&gt; 集合的删除db.collection.drop(); // 在当前数据库下的集合 12&gt; 查看集合show collections 集合创建集合创建 集合(collection)和table的区别? 创建固定大小的集合 12db.createCollection(&quot;mycol&quot;, &#123; capped : true, autoIndexId : true, size : 6142800, max : 10000 &#125; ) 文档文档的数据结构和JSON基本一样。所有存储在集合中的数据都是BSON格式。BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。 1234567db.col.insert(&#123;title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;) 查看已经插入的文档 1db.col.find() 也可以将数据定义为一个变量,然后插入 12345678document=(&#123;title: &apos;MongoDB 教程&apos;, description: &apos;MongoDB 是一个 Nosql 数据库&apos;, by: &apos;菜鸟教程&apos;, url: &apos;http://www.runoob.com&apos;, tags: [&apos;mongodb&apos;, &apos;database&apos;, &apos;NoSQL&apos;], likes: 100&#125;);db.col.insert(document); 文档更新语法MongoDB 更新文档 123456789101112131415161718192021222324252627282930db.collection.update( &lt;query&gt;, &lt;update&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;)例如:db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;)db.col.find().pretty()&#123; &quot;_id&quot; : ObjectId(&quot;56064f89ade2f21f36b03136&quot;), &quot;title&quot; : &quot;MongoDB&quot;, &quot;description&quot; : &quot;MongoDB 是一个 Nosql 数据库&quot;, &quot;by&quot; : &quot;菜鸟教程&quot;, &quot;url&quot; : &quot;http://www.runoob.com&quot;, &quot;tags&quot; : [ &quot;mongodb&quot;, &quot;database&quot;, &quot;NoSQL&quot; ], &quot;likes&quot; : 100&#125;以上语句只会修改第一条发现的文档，如果你要修改多条相同的文档，则需要设置 multi 参数为 true。&gt;db.col.update(&#123;&apos;title&apos;:&apos;MongoDB 教程&apos;&#125;,&#123;$set:&#123;&apos;title&apos;:&apos;MongoDB&apos;&#125;&#125;,&#123;multi:true&#125;) 删除文档MongoDB 删除文档 查询文档MongoDB 查询文档 MongoDB 条件操作符 MongoDB $type 操作符 MongoDB 索引MongoDB 索引 创建索引语法: 1db.collection.createIndex(keys, options) MongoDB JavaMongoDB Java","tags":[],"categories":[{"name":"数据处理","slug":"数据处理","permalink":"http://wiki.quartz.ren/categories/数据处理/"},{"name":"DataStore","slug":"数据处理/DataStore","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/"},{"name":"Mongo","slug":"数据处理/DataStore/Mongo","permalink":"http://wiki.quartz.ren/categories/数据处理/DataStore/Mongo/"}]},{"title":"Wiki Start","date":"2017-02-16T04:56:24.000Z","path":"wiki/WikiStart/","text":"https://notebook.zoho.com.cn 环境安装 安装nodejs 环境 https://www.quartz.ren/2017/10/12/nodejs-env/ 安装hexo 123456npm install -g hexo-cli...等好长时间ln -s /opt/node-v4.4.4-linux-x86/bin/hexo /usr/local/bin/hexo## 加速方案npm install -g hexo-cli --registry=http://registry.npm.taobao.org 创建hexo项目-建站 https://hexo.io/zh-cn/docs/setup 使用Wikitten主题 https://github.com/zthxxx/hexo-theme-Wikitten 主要的命令如下: 123456git clone https://github.com/zthxxx/hexo-theme-Wikitten.git themes/Wikittencp -rf themes/Wikitten/_source/* source/cp -rf themes/Wikitten/_scaffolds/* scaffolds/cp -f themes/Wikitten/_config.yml.example themes/Wikitten/_config.yml# edit and customize itvim themes/Wikitten/_config.yml 一些其他的配置 gitee 编辑环境 不错的vpn推荐 https://wiki.zthxxx.me/wiki/index/ Hexo Quick Start## Create a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"wiki","slug":"wiki","permalink":"http://wiki.quartz.ren/tags/wiki/"},{"name":"hexo","slug":"hexo","permalink":"http://wiki.quartz.ren/tags/hexo/"}],"categories":[]}]}